{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include required libs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_and_test_data():\n",
    "    train_features=pd.read_csv('TrainingSetValues.csv',parse_dates=True)\n",
    "    train_labels=pd.read_csv('TrainingSetLabels.csv')\n",
    "    to_predict_features=pd.read_csv('TestSetValues.csv',parse_dates=True)\n",
    "    \n",
    "    # merge training features and labels\n",
    "    #training_data = pd.merge(train_features, train_labels, how='inner', on=['id'])\n",
    "    \n",
    "    return train_features, train_labels, to_predict_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute median values for 0 amount_tsh\n",
    "def impute_missing_amount_tsh(df):\n",
    "    df.amount_tsh[df.amount_tsh <= 0] = np.median(df.amount_tsh[df.amount_tsh > 0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute median values for 0 gps height\n",
    "def impute_missing_gps_height(df):\n",
    "    df.gps_height[df.gps_height <= 0] = np.median(df.gps_height[df.gps_height > 0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute median values for 0 population\n",
    "def impute_missing_population(df):\n",
    "    df.population[df.population <= 0] = np.median(df.population[df.population > 0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform population into categories\n",
    "def label_population(row):\n",
    "    if row['population'] <=0:\n",
    "        return 'NA'\n",
    "    elif row['population'] >= 1 and row['population'] <= 40:\n",
    "        return 'A'\n",
    "    elif row['population'] >= 41 and row['population'] <= 67:\n",
    "        return 'B'\n",
    "    elif row['population'] >= 68 and row['population'] <= 99:\n",
    "        return 'C'\n",
    "    elif row['population'] >= 100 and row['population'] <= 131:\n",
    "        return 'D'\n",
    "    elif row['population'] >= 132 and row['population'] <= 175:\n",
    "        return 'E'\n",
    "    elif row['population'] >= 176 and row['population'] <= 219:\n",
    "        return 'F'\n",
    "    elif row['population'] >= 220 and row['population'] <= 259:\n",
    "        return 'G'\n",
    "    elif row['population'] >= 260 and row['population'] <= 349:\n",
    "        return 'H'\n",
    "    elif row['population'] >= 350 and row['population'] <= 448:\n",
    "        return 'I'\n",
    "    elif row['population'] >= 449 and row['population'] <= 598:\n",
    "        return 'J'\n",
    "    elif row['population'] >= 599 and row['population'] <= 1290:\n",
    "        return 'K'\n",
    "    elif row['population'] >= 1291:\n",
    "        return 'L'\n",
    "\n",
    "def transform_population_into_categories(df):\n",
    "    df['population_cat'] = df.apply(label_population, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute median values for construction year\n",
    "def impute_missing_construction_year(df):\n",
    "    df.construction_year[df.construction_year <= 0] = np.median(df.construction_year[df.construction_year > 0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute missing booleans with false and convert each value to float or integer\n",
    "def impute_missing_booleans(df, colname):\n",
    "    df[colname].fillna(False, inplace = True)\n",
    "    df[colname] = df[colname].apply(lambda x: float(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dates(X_train, X_test):\n",
    "    \"\"\"\n",
    "    date_recorded: this might be a useful variable for this analysis, although the year itself would be useless \n",
    "    in a practical scenario moving into the future. \n",
    "    We will convert this column into a datetime, and we will also \n",
    "    create 'year_recorded' and 'month_recorded' columns just in case those levels prove to be useful. \n",
    "    A visual inspection of both casts significant doubt on that possibility, but we'll proceed for now. \n",
    "    We will delete date_recorded itself, since random forest cannot accept datetime\n",
    "    \"\"\"\n",
    "    for i in [X_train, X_test]:\n",
    "        i['date_recorded'] = pd.to_datetime(i['date_recorded'])\n",
    "        i['year_recorded'] = i['date_recorded'].apply(lambda x: x.year)\n",
    "        i['month_recorded'] = i['date_recorded'].apply(lambda x: x.month)\n",
    "        i['date_recorded'] = (pd.to_datetime(i['date_recorded'])).apply(lambda x: x.toordinal())\n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since random forest doesnt work on datetime we will cconvert into month and year.\n",
    "# But, also if we simply convert month into numerical values, it doesnt work well because \n",
    "# there may be big distance between Jan and December and also between 1970 to  2010, to take an example\n",
    "# So its better to one hot encode them after transforming the date to month and year\n",
    "def transform_date_recorded_to_month_and_year(df):\n",
    "    df['date_recorded'] = pd.to_datetime(df['date_recorded'])\n",
    "    df['year_recorded'] = df['date_recorded'].apply(lambda x: x.year)\n",
    "    df['month_recorded'] = df['date_recorded'].apply(lambda x: x.month)\n",
    "    df['date_recorded'] = (pd.to_datetime(df['date_recorded'])).apply(lambda x: x.toordinal())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dates2(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Turn year_recorded and month_recorded into dummy variables\n",
    "    \"\"\"\n",
    "    for z in ['month_recorded', 'year_recorded']:\n",
    "        X_train[z] = X_train[z].apply(lambda x: str(x))\n",
    "        X_test[z] = X_test[z].apply(lambda x: str(x))\n",
    "        good_cols = [z+'_'+i for i in X_train[z].unique() if i in X_test[z].unique()]\n",
    "        X_train = pd.concat((X_train, pd.get_dummies(X_train[z], prefix = z)[good_cols]), axis = 1)\n",
    "        X_test = pd.concat((X_test, pd.get_dummies(X_test[z], prefix = z)[good_cols]), axis = 1)\n",
    "        del X_test[z]\n",
    "        del X_train[z]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One Hot encode year and month recorded (first convert the month and year to string beafore OHEing)\n",
    "# Also delete the original ones\n",
    "def ohe_month_and_year_recorded(df_train, df_test):\n",
    "    for col in ['month_recorded', 'year_recorded']:\n",
    "        df_train[col] = df_train[col].apply(lambda x: str(x))\n",
    "        df_test[col] = df_test[col].apply(lambda x: str(x))\n",
    "        ohe_cols_postfix = [col + '_' + i for i in df_train[col].unique() if i in df_test[col].unique()]\n",
    "        df_train = pd.concat((df_train, pd.get_dummies(df_train[col], prefix = col)[ohe_cols_postfix]), axis = 1)\n",
    "        df_test = pd.concat((df_test, pd.get_dummies(df_test[col], prefix = col)[ohe_cols_postfix]), axis = 1)\n",
    "        del df_test[col]\n",
    "        del df_train[col]\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def locs(X_train, X_test):\n",
    "    \"\"\"\n",
    "    fill in the nulls for ['longitude', 'latitude', 'gps_height', 'population'] by using means from \n",
    "    ['subvillage', 'district_code', 'basin'], and lastly the overall mean\n",
    "    \"\"\"\n",
    "    trans = ['longitude', 'latitude', 'gps_height', 'population']\n",
    "    for i in [X_train, X_test]:\n",
    "        i.loc[i.longitude == 0, 'latitude'] = 0\n",
    "    for z in trans:\n",
    "        for i in [X_train, X_test]:\n",
    "            i[z].replace(0., np.NaN, inplace = True)\n",
    "            i[z].replace(1., np.NaN, inplace = True)\n",
    "        \n",
    "        for j in ['subvillage', 'district_code', 'basin']:\n",
    "        \n",
    "            X_train['mean'] = X_train.groupby([j])[z].transform('mean')\n",
    "            X_train[z] = X_train[z].fillna(X_train['mean'])\n",
    "            o = X_train.groupby([j])[z].mean()\n",
    "            fill = pd.merge(X_test, pd.DataFrame(o), left_on=[j], right_index=True, how='left').iloc[:,-1]\n",
    "            X_test[z] = X_test[z].fillna(fill)\n",
    "        \n",
    "        X_train[z] = X_train[z].fillna(X_train[z].mean())\n",
    "        X_test[z] = X_test[z].fillna(X_train[z].mean())\n",
    "        del X_train['mean']\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to drop columns from a dataframe\n",
    "def drop_columns(df, cols_to_drop):\n",
    "    for col in cols_to_drop:\n",
    "        del df[col]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(p):\n",
    "    return 1-(p**2 + (1-p)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def small_n2(X_train, X_test):\n",
    "    cols = [i for i in X_train.columns if type(X_train[i].iloc[0]) == str]\n",
    "    X_train[cols] = X_train[cols].where(X_train[cols].apply(lambda x: x.map(x.value_counts())) > 100, \"other\")\n",
    "    for column in cols:\n",
    "        for i in X_test[column].unique():\n",
    "            if i not in X_train[column].unique():\n",
    "                X_test[column].replace(i, 'other', inplace=True)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda(X_train, X_test, y_train, cols=['population', 'gps_height', 'latitude', 'longitude']):\n",
    "    sc = StandardScaler()\n",
    "    X_train_std = sc.fit_transform(X_train[cols])\n",
    "    X_test_std = sc.transform(X_test[cols])\n",
    "    lda = LDA(n_components=None)\n",
    "    X_train_lda = lda.fit_transform(X_train_std, y_train.values.ravel())\n",
    "    X_test_lda = lda.transform(X_test_std)\n",
    "    X_train = pd.concat((pd.DataFrame(X_train_lda), X_train), axis=1)\n",
    "    X_test = pd.concat((pd.DataFrame(X_test_lda), X_test), axis=1)\n",
    "    for i in cols:\n",
    "        del X_train[i]\n",
    "        del X_test[i]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(df_train, df_test):\n",
    "    columns = [i for i in df_train.columns if type(df_train[i].iloc[0]) == str]\n",
    "    for column in columns:\n",
    "        df_train[column].fillna('NULL', inplace = True)\n",
    "        ohe_cols = [column+'_'+i for i in df_train[column].unique() if i in df_test[column].unique()]\n",
    "        df_train = pd.concat((df_train, pd.get_dummies(df_train[column], prefix = column)[ohe_cols]), axis = 1)\n",
    "        df_test = pd.concat((df_test, pd.get_dummies(df_test[column], prefix = column)[ohe_cols]), axis = 1)\n",
    "        del df_train[column]\n",
    "        del df_test[column]\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode1(df):\n",
    "    columns = [x for x in df.columns if type(df[x].iloc[0]) == str]\n",
    "    for column in columns:\n",
    "        df[column].fillna('NULL', inplace = True)\n",
    "        ohe_cols = [column + '_' + x for x in df[column].unique() if x in df[column].unique()]\n",
    "        df = pd.concat((df, pd.get_dummies(df[column], prefix = column)[ohe_cols]), axis = 1)\n",
    "        del df[column]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function for column values analysis\n",
    "def analyze_unique_values_for_column(df, colname):\n",
    "    unique_col_vals = df[colname].unique()\n",
    "    tmp_str = \"Unique \" + colname + \"s:\"\n",
    "    print(\"****************************\")\n",
    "    print(tmp_str, unique_col_vals.size)\n",
    "    print(\"****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function for column values analysis\n",
    "def analyze_in_detail_unique_values_for_column(df, colname):\n",
    "    unique_col_vals = df[colname].unique()\n",
    "    tmp_str = \"Unique \" + colname + \"s:\"\n",
    "    print(\"****************************\")\n",
    "    print(tmp_str, unique_col_vals.size)\n",
    "    print(\"****************************\")\n",
    "    lessthan10 = 0\n",
    "    lessthan20 = 0\n",
    "    lessthan30 = 0\n",
    "    lessthan50 = 0\n",
    "    lessthan100 = 0\n",
    "    for val in unique_col_vals:\n",
    "        cnt = df[df[colname] == val][colname].count()\n",
    "        print(val, cnt) # uncomment this line if you want to see the count of each colname-value\n",
    "        if(cnt < 10):\n",
    "            lessthan10 +=1     \n",
    "            print(val, cnt)\n",
    "        elif(cnt < 20):\n",
    "            lessthan20 +=1\n",
    "        elif(cnt < 30):\n",
    "            lessthan30 +=1\n",
    "        elif(cnt < 50):\n",
    "            lessthan50 +=1\n",
    "\n",
    "    print(\"lessthan50: \", lessthan50 )\n",
    "    print(\"lessthan30: \", lessthan30 )\n",
    "    print(\"lessthan20: \", lessthan20 )\n",
    "    print(\"lessthan10: \", lessthan10 )\n",
    "    print(\"****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data, df_training_labels, df_topredict_data = load_train_and_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Brief Stats of each column\n",
      "*****************************************\n",
      "                 id     amount_tsh    gps_height     longitude      latitude  \\\n",
      "count  59400.000000   59400.000000  59400.000000  59400.000000  5.940000e+04   \n",
      "mean   37115.131768     317.650385    668.297239     34.077427 -5.706033e+00   \n",
      "std    21453.128371    2997.574558    693.116350      6.567432  2.946019e+00   \n",
      "min        0.000000       0.000000    -90.000000      0.000000 -1.164944e+01   \n",
      "25%    18519.750000       0.000000      0.000000     33.090347 -8.540621e+00   \n",
      "50%    37061.500000       0.000000    369.000000     34.908743 -5.021597e+00   \n",
      "75%    55656.500000      20.000000   1319.250000     37.178387 -3.326156e+00   \n",
      "max    74247.000000  350000.000000   2770.000000     40.345193 -2.000000e-08   \n",
      "\n",
      "        num_private   region_code  district_code    population  \\\n",
      "count  59400.000000  59400.000000   59400.000000  59400.000000   \n",
      "mean       0.474141     15.297003       5.629747    179.909983   \n",
      "std       12.236230     17.587406       9.633649    471.482176   \n",
      "min        0.000000      1.000000       0.000000      0.000000   \n",
      "25%        0.000000      5.000000       2.000000      0.000000   \n",
      "50%        0.000000     12.000000       3.000000     25.000000   \n",
      "75%        0.000000     17.000000       5.000000    215.000000   \n",
      "max     1776.000000     99.000000      80.000000  30500.000000   \n",
      "\n",
      "       construction_year  \n",
      "count       59400.000000  \n",
      "mean         1300.652475  \n",
      "std           951.620547  \n",
      "min             0.000000  \n",
      "25%             0.000000  \n",
      "50%          1986.000000  \n",
      "75%          2004.000000  \n",
      "max          2013.000000  \n",
      "\n",
      "*****************************************\n",
      "number of nonzeros in each column\n",
      "*****************************************\n",
      "id                       59399\n",
      "amount_tsh               17761\n",
      "date_recorded            59400\n",
      "funder                   59400\n",
      "gps_height               38962\n",
      "installer                59400\n",
      "longitude                57588\n",
      "latitude                 59400\n",
      "wpt_name                 59400\n",
      "num_private                757\n",
      "basin                    59400\n",
      "subvillage               59400\n",
      "region                   59400\n",
      "region_code              59400\n",
      "district_code            59377\n",
      "lga                      59400\n",
      "ward                     59400\n",
      "population               38019\n",
      "public_meeting           54345\n",
      "recorded_by              59400\n",
      "scheme_management        59400\n",
      "scheme_name              59400\n",
      "permit                   41908\n",
      "construction_year        38691\n",
      "extraction_type          59400\n",
      "extraction_type_group    59400\n",
      "extraction_type_class    59400\n",
      "management               59400\n",
      "management_group         59400\n",
      "payment                  59400\n",
      "payment_type             59400\n",
      "water_quality            59400\n",
      "quality_group            59400\n",
      "quantity                 59400\n",
      "quantity_group           59400\n",
      "source                   59400\n",
      "source_type              59400\n",
      "source_class             59400\n",
      "waterpoint_type          59400\n",
      "waterpoint_type_group    59400\n",
      "dtype: int64\n",
      "\n",
      "*****************************************\n",
      "no. of nulls in each column\n",
      "*****************************************\n",
      "id                           0\n",
      "amount_tsh                   0\n",
      "date_recorded                0\n",
      "funder                    3635\n",
      "gps_height                   0\n",
      "installer                 3655\n",
      "longitude                    0\n",
      "latitude                     0\n",
      "wpt_name                     0\n",
      "num_private                  0\n",
      "basin                        0\n",
      "subvillage                 371\n",
      "region                       0\n",
      "region_code                  0\n",
      "district_code                0\n",
      "lga                          0\n",
      "ward                         0\n",
      "population                   0\n",
      "public_meeting            3334\n",
      "recorded_by                  0\n",
      "scheme_management         3877\n",
      "scheme_name              28166\n",
      "permit                    3056\n",
      "construction_year            0\n",
      "extraction_type              0\n",
      "extraction_type_group        0\n",
      "extraction_type_class        0\n",
      "management                   0\n",
      "management_group             0\n",
      "payment                      0\n",
      "payment_type                 0\n",
      "water_quality                0\n",
      "quality_group                0\n",
      "quantity                     0\n",
      "quantity_group               0\n",
      "source                       0\n",
      "source_type                  0\n",
      "source_class                 0\n",
      "waterpoint_type              0\n",
      "waterpoint_type_group        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"*****************************************\\nBrief Stats of each column\\n*****************************************\")\n",
    "print(df_training_data.describe())\n",
    "\n",
    "#getting number of nonzeros in each column\n",
    "print(\"\\n*****************************************\\nnumber of nonzeros in each column\\n*****************************************\")\n",
    "print(df_training_data.astype(bool).sum(axis=0))\n",
    "\n",
    "# getting no. of nulls in each column\n",
    "print(\"\\n*****************************************\\nno. of nulls in each column\\n*****************************************\")\n",
    "print(df_training_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze_unique_values_for_column(df_training_data, \"funder\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"installer\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"wpt_name\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"basin\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"subvillage\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"region\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"region_code\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"district_code\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"lga\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"ward\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"recorded_by\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"scheme_management\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"scheme_name\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"extraction_type\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"extraction_type_group\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"extraction_type_class\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"management\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"management_group\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"management_group\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"payment\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"payment_type\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"management_group\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"water_quality\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"quality_group\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"quantity\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"quantity_group\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"source\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"source_type\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"source_class\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"source_class\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"waterpoint_type\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"waterpoint_type_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze_in_detail_unique_values_for_column(df_training_data, \"population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze_in_detail_unique_values_for_column(df_training_data, \"management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze_in_detail_unique_values_for_column(df_training_data, \"management_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze_in_detail_unique_values_for_column(df_training_data, \"extraction_type_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_training_labels['id']\n",
    "\n",
    "#df_training_data, df_topredict_data = dates(df_training_data, df_topredict_data)\n",
    "df_training_data = transform_date_recorded_to_month_and_year(df_training_data)\n",
    "df_topredict_data = transform_date_recorded_to_month_and_year(df_topredict_data)\n",
    "\n",
    "df_training_data, df_topredict_data = ohe_month_and_year_recorded(df_training_data, df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# impute missing construction year with median construction year\n",
    "df_training_data = impute_missing_construction_year(df_training_data)\n",
    "df_topredict_data = impute_missing_construction_year(df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the fields public_meeting and permit are boolean, but there are many missing values \n",
    "# (3334 in public_meeting and 3056 in permit)\n",
    "# impute these missing values with FALSE\n",
    "df_training_data = impute_missing_booleans(df_training_data, \"public_meeting\")\n",
    "df_topredict_data = impute_missing_booleans(df_topredict_data, \"public_meeting\")\n",
    "\n",
    "df_training_data = impute_missing_booleans(df_training_data, \"permit\")\n",
    "df_topredict_data = impute_missing_booleans(df_topredict_data, \"permit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fill in the nulls for ['longitude', 'latitude', 'gps_height', 'population'] by using means from \n",
    "['subvillage', 'district_code', 'basin'], and lastly the overall mean\n",
    "\"\"\"\n",
    "df_training_data, df_topredict_data = locs(df_training_data, df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data['population'] = np.log(df_training_data['population'])\n",
    "df_topredict_data['population'] = np.log(df_topredict_data['population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop unwanted columns \n",
    "columns_to_drop = ['id','amount_tsh',  'num_private', 'region', 'quantity', 'quality_group', 'source_type', 'payment', \n",
    "'waterpoint_type_group', 'extraction_type_group', 'recorded_by']\n",
    "df_training_data = drop_columns(df_training_data, columns_to_drop)\n",
    "df_topredict_data = drop_columns(df_topredict_data, columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data, df_topredict_data = small_n2(df_training_data, df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data, df_topredict_data = lda(df_training_data, df_topredict_data, df_training_labels, cols = ['gps_height', 'latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encode the columns that have string values\n",
    "df_training_data,df_topredict_data = one_hot_encode(df_training_data, df_topredict_data)\n",
    "#df_topredict_data = one_hot_encode1(df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537\n"
     ]
    }
   ],
   "source": [
    "print(len(df_training_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8122\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(criterion='gini', min_samples_split=6, n_estimators=1000, max_features='auto',\n",
    "     oob_score=True, random_state=1, n_jobs=-1)\n",
    "rf.fit(df_training_data, df_training_labels.values.ravel())\n",
    "print (\"%.4f\" % rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = rf.predict(df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14820</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14821</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14822</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14823</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14824</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14825</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14826</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14827</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14828</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14829</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14830</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14831</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14832</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14833</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14834</th>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14835</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14836</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14837</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14838</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14839</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14840</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14841</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14842</th>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14843</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14844</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14845</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14846</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14847</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14848</th>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14849</th>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14850 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0\n",
       "0               non functional\n",
       "1                   functional\n",
       "2                   functional\n",
       "3               non functional\n",
       "4                   functional\n",
       "5                   functional\n",
       "6                   functional\n",
       "7               non functional\n",
       "8               non functional\n",
       "9                   functional\n",
       "10                  functional\n",
       "11              non functional\n",
       "12              non functional\n",
       "13              non functional\n",
       "14                  functional\n",
       "15                  functional\n",
       "16                  functional\n",
       "17              non functional\n",
       "18                  functional\n",
       "19              non functional\n",
       "20                  functional\n",
       "21              non functional\n",
       "22              non functional\n",
       "23              non functional\n",
       "24                  functional\n",
       "25                  functional\n",
       "26              non functional\n",
       "27              non functional\n",
       "28     functional needs repair\n",
       "29                  functional\n",
       "...                        ...\n",
       "14820           non functional\n",
       "14821               functional\n",
       "14822           non functional\n",
       "14823               functional\n",
       "14824               functional\n",
       "14825           non functional\n",
       "14826               functional\n",
       "14827           non functional\n",
       "14828           non functional\n",
       "14829               functional\n",
       "14830           non functional\n",
       "14831               functional\n",
       "14832               functional\n",
       "14833               functional\n",
       "14834  functional needs repair\n",
       "14835               functional\n",
       "14836               functional\n",
       "14837           non functional\n",
       "14838               functional\n",
       "14839               functional\n",
       "14840               functional\n",
       "14841               functional\n",
       "14842  functional needs repair\n",
       "14843               functional\n",
       "14844               functional\n",
       "14845           non functional\n",
       "14846               functional\n",
       "14847               functional\n",
       "14848               functional\n",
       "14849           non functional\n",
       "\n",
       "[14850 rows x 1 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['id','status_group']\n",
    "df_submission = pd.DataFrame(columns=columns)\n",
    "to_predict_features=pd.read_csv('TestSetValues.csv',parse_dates=True)\n",
    "to_predict_features = to_predict_features.reset_index(drop=True)\n",
    "df_predictions = df_predictions.reset_index(drop=True)\n",
    "df_submission = df_submission.reset_index(drop=True)\n",
    "df_submission['id'] = to_predict_features['id']\n",
    "df_submission['status_group'] = df_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of           id             status_group\n",
       "0      50785           non functional\n",
       "1      51630               functional\n",
       "2      17168               functional\n",
       "3      45559           non functional\n",
       "4      49871               functional\n",
       "5      52449               functional\n",
       "6      24806               functional\n",
       "7      28965           non functional\n",
       "8      36301           non functional\n",
       "9      54122               functional\n",
       "10       419               functional\n",
       "11     45750           non functional\n",
       "12       653           non functional\n",
       "13     14017           non functional\n",
       "14     44607               functional\n",
       "15     40228               functional\n",
       "16     27714               functional\n",
       "17     28785           non functional\n",
       "18     28330               functional\n",
       "19     18532           non functional\n",
       "20     69961               functional\n",
       "21     55083           non functional\n",
       "22      8691           non functional\n",
       "23     30331           non functional\n",
       "24     70970               functional\n",
       "25     61136               functional\n",
       "26     28799           non functional\n",
       "27     46825           non functional\n",
       "28     44718  functional needs repair\n",
       "29     37350               functional\n",
       "...      ...                      ...\n",
       "14820  52228           non functional\n",
       "14821  70038               functional\n",
       "14822  25901           non functional\n",
       "14823  21131               functional\n",
       "14824  26580               functional\n",
       "14825  66059           non functional\n",
       "14826  32944               functional\n",
       "14827  13686           non functional\n",
       "14828   8471           non functional\n",
       "14829  19620               functional\n",
       "14830  74162           non functional\n",
       "14831  37994               functional\n",
       "14832  71151               functional\n",
       "14833  45017               functional\n",
       "14834  12592  functional needs repair\n",
       "14835  58693               functional\n",
       "14836  57539               functional\n",
       "14837  71252           non functional\n",
       "14838   7869               functional\n",
       "14839  57316               functional\n",
       "14840  59757               functional\n",
       "14841  64579               functional\n",
       "14842  57731  functional needs repair\n",
       "14843  65541               functional\n",
       "14844  68174               functional\n",
       "14845  39307           non functional\n",
       "14846  18990               functional\n",
       "14847  28749               functional\n",
       "14848  33492               functional\n",
       "14849  68707           non functional\n",
       "\n",
       "[14850 rows x 2 columns]>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14850, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission.to_csv(\"submission_ab_kn.csv\", sep=\",\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert string labels to numerics\n",
    "#label_map = {\"functional\": 1, \"functional needs repair\": 2, \"non functional\": 3}\n",
    "#df_training_data['status_group_num']= df_training_data['status_group'].map(label_map).astype(int)\n",
    "\n",
    "# do sanity check\n",
    "#df_training_data[['id', 'amount_tsh', 'status_group', 'status_group_num']].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the training labels into a df\n",
    "#df_training_labels_str = df_training_data['status_group']\n",
    "#df_training_labels_num = np.array(df_training_data['status_group_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop the training labels from the training set\n",
    "#df_training_data= df_training_data.drop('status_group', axis = 1)\n",
    "#df_training_data= df_training_data.drop('status_group_num', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def one_hot_encode(X_train, X_test):\n",
    "#    columns = [i for i in X_train.columns if type(X_train[i].iloc[0]) == str]\n",
    "#    for column in columns:\n",
    "#        X_train[column].fillna('NULL', inplace = True)\n",
    "#        good_cols = [column+'_'+i for i in X_train[column].unique() if i in X_test[column].unique()]\n",
    "#        X_train = pd.concat((X_train, pd.get_dummies(X_train[column], prefix = column)[good_cols]), axis = 1)\n",
    "#        X_test = pd.concat((X_test, pd.get_dummies(X_test[column], prefix = column)[good_cols]), axis = 1)\n",
    "#        del X_train[column]\n",
    "#        del X_test[column]\n",
    "#    return X_train, X_test\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing population \n",
    "#df_training_data = impute_missing_population1(df_training_data)\n",
    "#df_topredict_data = impute_missing_population1(df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze_in_detail_unique_values_for_column(df_training_data, 'population_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train, X_test, y_train, y_test = \\n    train_test_split(df_training_data, df_training_labels_num, test_size = 0.25, random_state = 42)\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = \n",
    "    train_test_split(df_training_data, df_training_labels_num, test_size = 0.25, random_state = 42)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfeatures_to_consider =\\n    ['amount_tsh', 'gps_height', 'longitude', 'latitude', 'region_code', 'district_code', 'population', 'construction_year']\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "features_to_consider =\n",
    "    ['amount_tsh', 'gps_height', 'longitude', 'latitude', 'region_code', 'district_code', 'population', 'construction_year']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#def run_random_forest_predictor(X_train1, y_train1, X_test1, y_test1, features_to_consider):\\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\\n# Train the model on training data\\ndf1 = X_train[features_to_consider]\\nrf.fit(df1, y_train)\\npredictions = rf.predict(X_test[features_to_consider])\\n# Calculate the absolute errors\\nerrors = abs(predictions - y_test)\\n# Print out the mean absolute error (mae)\\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\\n\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#def run_random_forest_predictor(X_train1, y_train1, X_test1, y_test1, features_to_consider):\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "df1 = X_train[features_to_consider]\n",
    "rf.fit(df1, y_train)\n",
    "predictions = rf.predict(X_test[features_to_consider])\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
