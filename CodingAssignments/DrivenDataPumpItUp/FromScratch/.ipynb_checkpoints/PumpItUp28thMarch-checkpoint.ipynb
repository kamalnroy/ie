{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include required libs\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.svm import SVC\n",
    "import random\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import preprocessing\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_and_test_data():\n",
    "    train_features=pd.read_csv('TrainingSetValues.csv',parse_dates=True)\n",
    "    train_labels=pd.read_csv('TrainingSetLabels.csv')\n",
    "    to_predict_features=pd.read_csv('TestSetValues.csv',parse_dates=True)\n",
    "    \n",
    "    # merge training features and labels\n",
    "    #training_data = pd.merge(train_features, train_labels, how='inner', on=['id'])\n",
    "    \n",
    "    return train_features, train_labels, to_predict_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data, df_training_labels, df_topredict_data = load_train_and_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert string labels to numerics\n",
    "#label_map = {\"functional\": 1, \"functional needs repair\": 2, \"non functional\": 3}\n",
    "#df_training_data['status_group_num']= df_training_data['status_group'].map(label_map).astype(int)\n",
    "\n",
    "# do sanity check\n",
    "#df_training_data[['id', 'amount_tsh', 'status_group', 'status_group_num']].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the training labels into a df\n",
    "#df_training_labels_str = df_training_data['status_group']\n",
    "#df_training_labels_num = np.array(df_training_data['status_group_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop the training labels from the training set\n",
    "#df_training_data= df_training_data.drop('status_group', axis = 1)\n",
    "#df_training_data= df_training_data.drop('status_group_num', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Brief Stats of each column\n",
      "*****************************************\n",
      "                 id     amount_tsh    gps_height     longitude      latitude  \\\n",
      "count  59400.000000   59400.000000  59400.000000  59400.000000  5.940000e+04   \n",
      "mean   37115.131768     317.650385    668.297239     34.077427 -5.706033e+00   \n",
      "std    21453.128371    2997.574558    693.116350      6.567432  2.946019e+00   \n",
      "min        0.000000       0.000000    -90.000000      0.000000 -1.164944e+01   \n",
      "25%    18519.750000       0.000000      0.000000     33.090347 -8.540621e+00   \n",
      "50%    37061.500000       0.000000    369.000000     34.908743 -5.021597e+00   \n",
      "75%    55656.500000      20.000000   1319.250000     37.178387 -3.326156e+00   \n",
      "max    74247.000000  350000.000000   2770.000000     40.345193 -2.000000e-08   \n",
      "\n",
      "        num_private   region_code  district_code    population  \\\n",
      "count  59400.000000  59400.000000   59400.000000  59400.000000   \n",
      "mean       0.474141     15.297003       5.629747    179.909983   \n",
      "std       12.236230     17.587406       9.633649    471.482176   \n",
      "min        0.000000      1.000000       0.000000      0.000000   \n",
      "25%        0.000000      5.000000       2.000000      0.000000   \n",
      "50%        0.000000     12.000000       3.000000     25.000000   \n",
      "75%        0.000000     17.000000       5.000000    215.000000   \n",
      "max     1776.000000     99.000000      80.000000  30500.000000   \n",
      "\n",
      "       construction_year  \n",
      "count       59400.000000  \n",
      "mean         1300.652475  \n",
      "std           951.620547  \n",
      "min             0.000000  \n",
      "25%             0.000000  \n",
      "50%          1986.000000  \n",
      "75%          2004.000000  \n",
      "max          2013.000000  \n",
      "\n",
      "*****************************************\n",
      "number of nonzeros in each column\n",
      "*****************************************\n",
      "id                       59399\n",
      "amount_tsh               17761\n",
      "date_recorded            59400\n",
      "funder                   59400\n",
      "gps_height               38962\n",
      "installer                59400\n",
      "longitude                57588\n",
      "latitude                 59400\n",
      "wpt_name                 59400\n",
      "num_private                757\n",
      "basin                    59400\n",
      "subvillage               59400\n",
      "region                   59400\n",
      "region_code              59400\n",
      "district_code            59377\n",
      "lga                      59400\n",
      "ward                     59400\n",
      "population               38019\n",
      "public_meeting           54345\n",
      "recorded_by              59400\n",
      "scheme_management        59400\n",
      "scheme_name              59400\n",
      "permit                   41908\n",
      "construction_year        38691\n",
      "extraction_type          59400\n",
      "extraction_type_group    59400\n",
      "extraction_type_class    59400\n",
      "management               59400\n",
      "management_group         59400\n",
      "payment                  59400\n",
      "payment_type             59400\n",
      "water_quality            59400\n",
      "quality_group            59400\n",
      "quantity                 59400\n",
      "quantity_group           59400\n",
      "source                   59400\n",
      "source_type              59400\n",
      "source_class             59400\n",
      "waterpoint_type          59400\n",
      "waterpoint_type_group    59400\n",
      "dtype: int64\n",
      "\n",
      "*****************************************\n",
      "no. of nulls in each column\n",
      "*****************************************\n",
      "id                           0\n",
      "amount_tsh                   0\n",
      "date_recorded                0\n",
      "funder                    3635\n",
      "gps_height                   0\n",
      "installer                 3655\n",
      "longitude                    0\n",
      "latitude                     0\n",
      "wpt_name                     0\n",
      "num_private                  0\n",
      "basin                        0\n",
      "subvillage                 371\n",
      "region                       0\n",
      "region_code                  0\n",
      "district_code                0\n",
      "lga                          0\n",
      "ward                         0\n",
      "population                   0\n",
      "public_meeting            3334\n",
      "recorded_by                  0\n",
      "scheme_management         3877\n",
      "scheme_name              28166\n",
      "permit                    3056\n",
      "construction_year            0\n",
      "extraction_type              0\n",
      "extraction_type_group        0\n",
      "extraction_type_class        0\n",
      "management                   0\n",
      "management_group             0\n",
      "payment                      0\n",
      "payment_type                 0\n",
      "water_quality                0\n",
      "quality_group                0\n",
      "quantity                     0\n",
      "quantity_group               0\n",
      "source                       0\n",
      "source_type                  0\n",
      "source_class                 0\n",
      "waterpoint_type              0\n",
      "waterpoint_type_group        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"*****************************************\\nBrief Stats of each column\\n*****************************************\")\n",
    "print(df_training_data.describe())\n",
    "\n",
    "#getting number of nonzeros in each column\n",
    "print(\"\\n*****************************************\\nnumber of nonzeros in each column\\n*****************************************\")\n",
    "print(df_training_data.astype(bool).sum(axis=0))\n",
    "\n",
    "# getting no. of nulls in each column\n",
    "print(\"\\n*****************************************\\nno. of nulls in each column\\n*****************************************\")\n",
    "print(df_training_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute median values for 0 amount_tsh\n",
    "def impute_missing_amount_tsh(df):\n",
    "    df.amount_tsh[df.amount_tsh <= 0] = np.median(df.amount_tsh[df.amount_tsh > 0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute median values for 0 gps height\n",
    "def impute_missing_gps_height(df):\n",
    "    df.gps_height[df.gps_height <= 0] = np.median(df.gps_height[df.gps_height > 0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute median values for 0 population\n",
    "def impute_missing_population(df):\n",
    "    df.population[df.population <= 0] = np.median(df.population[df.population > 0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute median values for construction year\n",
    "def impute_missing_construction_year(df):\n",
    "    df.construction_year[df.construction_year <= 0] = np.median(df.construction_year[df.construction_year > 0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute missing booleans with false and convert each value to float or integer\n",
    "def impute_missing_booleans(df, colname):\n",
    "    df[colname].fillna(False, inplace = True)\n",
    "    df[colname] = df[colname].apply(lambda x: float(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dates(X_train, X_test):\n",
    "    \"\"\"\n",
    "    date_recorded: this might be a useful variable for this analysis, although the year itself would be useless in a practical scenario moving into the future. We will convert this column into a datetime, and we will also create 'year_recorded' and 'month_recorded' columns just in case those levels prove to be useful. A visual inspection of both casts significant doubt on that possibility, but we'll proceed for now. We will delete date_recorded itself, since random forest cannot accept datetime\n",
    "    \"\"\"\n",
    "    for i in [X_train, X_test]:\n",
    "        i['date_recorded'] = pd.to_datetime(i['date_recorded'])\n",
    "        i['year_recorded'] = i['date_recorded'].apply(lambda x: x.year)\n",
    "        i['month_recorded'] = i['date_recorded'].apply(lambda x: x.month)\n",
    "        i['date_recorded'] = (pd.to_datetime(i['date_recorded'])).apply(lambda x: x.toordinal())\n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dates2(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Turn year_recorded and month_recorded into dummy variables\n",
    "    \"\"\"\n",
    "    for z in ['month_recorded', 'year_recorded']:\n",
    "        X_train[z] = X_train[z].apply(lambda x: str(x))\n",
    "        X_test[z] = X_test[z].apply(lambda x: str(x))\n",
    "        good_cols = [z+'_'+i for i in X_train[z].unique() if i in X_test[z].unique()]\n",
    "        X_train = pd.concat((X_train, pd.get_dummies(X_train[z], prefix = z)[good_cols]), axis = 1)\n",
    "        X_test = pd.concat((X_test, pd.get_dummies(X_test[z], prefix = z)[good_cols]), axis = 1)\n",
    "        del X_test[z]\n",
    "        del X_train[z]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def locs(X_train, X_test):\n",
    "    \"\"\"\n",
    "    fill in the nulls for ['longitude', 'latitude', 'gps_height', 'population'] by using means from \n",
    "    ['subvillage', 'district_code', 'basin'], and lastly the overall mean\n",
    "    \"\"\"\n",
    "    trans = ['longitude', 'latitude', 'gps_height', 'population']\n",
    "    for i in [X_train, X_test]:\n",
    "        i.loc[i.longitude == 0, 'latitude'] = 0\n",
    "    for z in trans:\n",
    "        for i in [X_train, X_test]:\n",
    "            i[z].replace(0., np.NaN, inplace = True)\n",
    "            i[z].replace(1., np.NaN, inplace = True)\n",
    "        \n",
    "        for j in ['subvillage', 'district_code', 'basin']:\n",
    "        \n",
    "            X_train['mean'] = X_train.groupby([j])[z].transform('mean')\n",
    "            X_train[z] = X_train[z].fillna(X_train['mean'])\n",
    "            o = X_train.groupby([j])[z].mean()\n",
    "            fill = pd.merge(X_test, pd.DataFrame(o), left_on=[j], right_index=True, how='left').iloc[:,-1]\n",
    "            X_test[z] = X_test[z].fillna(fill)\n",
    "        \n",
    "        X_train[z] = X_train[z].fillna(X_train[z].mean())\n",
    "        X_test[z] = X_test[z].fillna(X_train[z].mean())\n",
    "        del X_train['mean']\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_unwanted_columns(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Here we define all the columns that we want to delete right off the bat.\n",
    "    # id: we drop the id column because it is not a useful predictor.\n",
    "    # 'amount_tsh' is mostly blank - delete\n",
    "    # wpt_name: not useful, delete (too many values)\n",
    "    # subvillage: too many values, delete\n",
    "    # scheme_name: this is almost 50% nulls, so we will delete this column\n",
    "    # num_private: we will delete this column because ~99% of the values are zeros.\n",
    "    # region: drop this b/c is seems very similar to region_code, though not 100% sure about this one!\n",
    "    \"\"\"\n",
    "    z = ['id','amount_tsh',  'num_private', 'region', \n",
    "          'quantity', 'quality_group', 'source_type', 'payment', \n",
    "          'waterpoint_type_group',\n",
    "         'extraction_type_group']\n",
    "    for i in z:\n",
    "        del X_train[i]\n",
    "        del X_test[i]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(p):\n",
    "    return 1-(p**2 + (1-p)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def small_n2(X_train, X_test):\n",
    "    cols = [i for i in X_train.columns if type(X_train[i].iloc[0]) == str]\n",
    "    X_train[cols] = X_train[cols].where(X_train[cols].apply(lambda x: x.map(x.value_counts())) > 100, \"other\")\n",
    "    for column in cols:\n",
    "        for i in X_test[column].unique():\n",
    "            if i not in X_train[column].unique():\n",
    "                X_test[column].replace(i, 'other', inplace=True)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda(X_train, X_test, y_train, cols=['population', 'gps_height', 'latitude', 'longitude']):\n",
    "    sc = StandardScaler()\n",
    "    X_train_std = sc.fit_transform(X_train[cols])\n",
    "    X_test_std = sc.transform(X_test[cols])\n",
    "    lda = LDA(n_components=None)\n",
    "    X_train_lda = lda.fit_transform(X_train_std, y_train.values.ravel())\n",
    "    X_test_lda = lda.transform(X_test_std)\n",
    "    X_train = pd.concat((pd.DataFrame(X_train_lda), X_train), axis=1)\n",
    "    X_test = pd.concat((pd.DataFrame(X_test_lda), X_test), axis=1)\n",
    "    for i in cols:\n",
    "        del X_train[i]\n",
    "        del X_test[i]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(X_train, X_test):\n",
    "    columns = [i for i in X_train.columns if type(X_train[i].iloc[0]) == str]\n",
    "    for column in columns:\n",
    "        X_train[column].fillna('NULL', inplace = True)\n",
    "        good_cols = [column+'_'+i for i in X_train[column].unique() if i in X_test[column].unique()]\n",
    "        X_train = pd.concat((X_train, pd.get_dummies(X_train[column], prefix = column)[good_cols]), axis = 1)\n",
    "        X_test = pd.concat((X_test, pd.get_dummies(X_test[column], prefix = column)[good_cols]), axis = 1)\n",
    "        del X_train[column]\n",
    "        del X_test[column]\n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_unique_values_for_column(df, colname):\n",
    "    unique_col_vals = df[colname].unique()\n",
    "    tmp_str = \"Unique \" + colname + \"s:\"\n",
    "    print(\"****************************\")\n",
    "    print(tmp_str, unique_col_vals.size)\n",
    "    print(\"****************************\")\n",
    "#    lessthan10 = 0\n",
    "#    lessthan20 = 0\n",
    "#    lessthan30 = 0\n",
    "#    lessthan50 = 0\n",
    "#    lessthan100 = 0\n",
    "#    for val in unique_col_vals:\n",
    "#        cnt = df[df[colname] == val][colname].count()\n",
    "#        #print(val, cnt) # uncomment this line if you want to see the count of each colname-value\n",
    "#        if(cnt < 10):\n",
    "#            lessthan10 +=1        \n",
    "#        elif(cnt < 20):\n",
    "#            lessthan20 +=1\n",
    "#        elif(cnt < 30):\n",
    "#            lessthan30 +=1\n",
    "#        elif(cnt < 50):\n",
    "#            lessthan50 +=1\n",
    "#\n",
    "#    print(\"lessthan50: \", lessthan50 )\n",
    "#    print(\"lessthan30: \", lessthan30 )\n",
    "#    print(\"lessthan20: \", lessthan20 )\n",
    "#    print(\"lessthan10: \", lessthan10 )\n",
    "#    print(\"****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************\n",
      "Unique funders: 1898\n",
      "****************************\n",
      "****************************\n",
      "Unique installers: 2146\n",
      "****************************\n",
      "****************************\n",
      "Unique wpt_names: 37400\n",
      "****************************\n",
      "****************************\n",
      "Unique basins: 9\n",
      "****************************\n",
      "****************************\n",
      "Unique subvillages: 19288\n",
      "****************************\n",
      "****************************\n",
      "Unique regions: 21\n",
      "****************************\n",
      "****************************\n",
      "Unique region_codes: 27\n",
      "****************************\n",
      "****************************\n",
      "Unique lgas: 125\n",
      "****************************\n",
      "****************************\n",
      "Unique wards: 2092\n",
      "****************************\n",
      "****************************\n",
      "Unique recorded_bys: 1\n",
      "****************************\n",
      "****************************\n",
      "Unique scheme_managements: 13\n",
      "****************************\n",
      "****************************\n",
      "Unique scheme_names: 2697\n",
      "****************************\n",
      "****************************\n",
      "Unique extraction_types: 18\n",
      "****************************\n",
      "****************************\n",
      "Unique extraction_type_groups: 13\n",
      "****************************\n",
      "****************************\n",
      "Unique extraction_type_classs: 7\n",
      "****************************\n",
      "****************************\n",
      "Unique managements: 12\n",
      "****************************\n",
      "****************************\n",
      "Unique management_groups: 5\n",
      "****************************\n",
      "****************************\n",
      "Unique management_groups: 5\n",
      "****************************\n",
      "****************************\n",
      "Unique payments: 7\n",
      "****************************\n",
      "****************************\n",
      "Unique payment_types: 7\n",
      "****************************\n",
      "****************************\n",
      "Unique management_groups: 5\n",
      "****************************\n",
      "****************************\n",
      "Unique water_qualitys: 8\n",
      "****************************\n",
      "****************************\n",
      "Unique quality_groups: 6\n",
      "****************************\n",
      "****************************\n",
      "Unique quantitys: 5\n",
      "****************************\n",
      "****************************\n",
      "Unique quantity_groups: 5\n",
      "****************************\n",
      "****************************\n",
      "Unique sources: 10\n",
      "****************************\n",
      "****************************\n",
      "Unique source_types: 7\n",
      "****************************\n",
      "****************************\n",
      "Unique source_classs: 3\n",
      "****************************\n",
      "****************************\n",
      "Unique source_classs: 3\n",
      "****************************\n",
      "****************************\n",
      "Unique waterpoint_types: 7\n",
      "****************************\n",
      "****************************\n",
      "Unique waterpoint_type_groups: 6\n",
      "****************************\n"
     ]
    }
   ],
   "source": [
    "analyze_unique_values_for_column(df_training_data, \"funder\")\n",
    "analyze_unique_values_for_column(df_training_data, \"installer\")\n",
    "analyze_unique_values_for_column(df_training_data, \"wpt_name\")\n",
    "analyze_unique_values_for_column(df_training_data, \"basin\")\n",
    "analyze_unique_values_for_column(df_training_data, \"subvillage\")\n",
    "analyze_unique_values_for_column(df_training_data, \"region\")\n",
    "analyze_unique_values_for_column(df_training_data, \"region_code\")\n",
    "analyze_unique_values_for_column(df_training_data, \"district_code\")\n",
    "analyze_unique_values_for_column(df_training_data, \"lga\")\n",
    "analyze_unique_values_for_column(df_training_data, \"ward\")\n",
    "analyze_unique_values_for_column(df_training_data, \"recorded_by\")\n",
    "analyze_unique_values_for_column(df_training_data, \"scheme_management\")\n",
    "analyze_unique_values_for_column(df_training_data, \"scheme_name\")\n",
    "analyze_unique_values_for_column(df_training_data, \"extraction_type\")\n",
    "analyze_unique_values_for_column(df_training_data, \"extraction_type_group\")\n",
    "analyze_unique_values_for_column(df_training_data, \"extraction_type_class\")\n",
    "analyze_unique_values_for_column(df_training_data, \"management\")\n",
    "analyze_unique_values_for_column(df_training_data, \"management_group\")\n",
    "analyze_unique_values_for_column(df_training_data, \"management_group\")\n",
    "analyze_unique_values_for_column(df_training_data, \"payment\")\n",
    "analyze_unique_values_for_column(df_training_data, \"payment_type\")\n",
    "analyze_unique_values_for_column(df_training_data, \"management_group\")\n",
    "analyze_unique_values_for_column(df_training_data, \"water_quality\")\n",
    "analyze_unique_values_for_column(df_training_data, \"quality_group\")\n",
    "analyze_unique_values_for_column(df_training_data, \"quantity\")\n",
    "analyze_unique_values_for_column(df_training_data, \"quantity_group\")\n",
    "analyze_unique_values_for_column(df_training_data, \"source\")\n",
    "analyze_unique_values_for_column(df_training_data, \"source_type\")\n",
    "analyze_unique_values_for_column(df_training_data, \"source_class\")\n",
    "analyze_unique_values_for_column(df_training_data, \"source_class\")\n",
    "analyze_unique_values_for_column(df_training_data, \"waterpoint_type\")\n",
    "analyze_unique_values_for_column(df_training_data, \"waterpoint_type_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_in_detail_unique_values_for_column(df, colname):\n",
    "    unique_col_vals = df[colname].unique()\n",
    "    tmp_str = \"Unique \" + colname + \"s:\"\n",
    "    print(\"****************************\")\n",
    "    print(tmp_str, unique_col_vals.size)\n",
    "    print(\"****************************\")\n",
    "    lessthan10 = 0\n",
    "    lessthan20 = 0\n",
    "    lessthan30 = 0\n",
    "    lessthan50 = 0\n",
    "    lessthan100 = 0\n",
    "    for val in unique_col_vals:\n",
    "        cnt = df[df[colname] == val][colname].count()\n",
    "        print(val, cnt) # uncomment this line if you want to see the count of each colname-value\n",
    "        if(cnt < 10):\n",
    "            lessthan10 +=1        \n",
    "        elif(cnt < 20):\n",
    "            lessthan20 +=1\n",
    "        elif(cnt < 30):\n",
    "            lessthan30 +=1\n",
    "        elif(cnt < 50):\n",
    "            lessthan50 +=1\n",
    "\n",
    "    print(\"lessthan50: \", lessthan50 )\n",
    "    print(\"lessthan30: \", lessthan30 )\n",
    "    print(\"lessthan20: \", lessthan20 )\n",
    "    print(\"lessthan10: \", lessthan10 )\n",
    "    print(\"****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************\n",
      "Unique managements: 12\n",
      "****************************\n",
      "vwc 40507\n",
      "wug 6515\n",
      "other 844\n",
      "private operator 1971\n",
      "water board 2933\n",
      "wua 2535\n",
      "company 685\n",
      "water authority 904\n",
      "parastatal 1768\n",
      "unknown 561\n",
      "other - school 99\n",
      "trust 78\n",
      "lessthan50:  0\n",
      "lessthan30:  0\n",
      "lessthan20:  0\n",
      "lessthan10:  0\n",
      "****************************\n"
     ]
    }
   ],
   "source": [
    "analyze_in_detail_unique_values_for_column(df_training_data, \"management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************\n",
      "Unique management_groups: 5\n",
      "****************************\n",
      "user-group 52490\n",
      "other 943\n",
      "commercial 3638\n",
      "parastatal 1768\n",
      "unknown 561\n",
      "lessthan50:  0\n",
      "lessthan30:  0\n",
      "lessthan20:  0\n",
      "lessthan10:  0\n",
      "****************************\n"
     ]
    }
   ],
   "source": [
    "analyze_in_detail_unique_values_for_column(df_training_data, \"management_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************\n",
      "Unique source_classs: 3\n",
      "****************************\n",
      "groundwater 45794\n",
      "surface 13328\n",
      "unknown 278\n",
      "lessthan50:  0\n",
      "lessthan30:  0\n",
      "lessthan20:  0\n",
      "lessthan10:  0\n",
      "****************************\n"
     ]
    }
   ],
   "source": [
    "analyze_in_detail_unique_values_for_column(df_training_data, \"extraction_type_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_training_labels['id']\n",
    "df_training_data, df_topredict_data = dates(df_training_data, df_topredict_data)\n",
    "df_training_data, df_topredict_data = dates2(df_training_data, df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing construction year with median construction year\n",
    "df_training_data = impute_missing_construction_year(df_training_data)\n",
    "df_topredict_data = impute_missing_construction_year(df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the fields public_meeting and permit are boolean, but there are many missing values (3334 in public_meeting and 3056 in permit)\n",
    "# impute these missing values with FALSE\n",
    "df_training_data = impute_missing_booleans(df_training_data, \"public_meeting\")\n",
    "df_topredict_data = impute_missing_booleans(df_topredict_data, \"public_meeting\")\n",
    "\n",
    "df_training_data = impute_missing_booleans(df_training_data, \"permit\")\n",
    "df_topredict_data = impute_missing_booleans(df_topredict_data, \"permit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fill in the nulls for ['longitude', 'latitude', 'gps_height', 'population'] by using means from \n",
    "['subvillage', 'district_code', 'basin'], and lastly the overall mean\n",
    "\"\"\"\n",
    "df_training_data, df_topredict_data = locs(df_training_data, df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data['population'] = np.log(df_training_data['population'])\n",
    "df_topredict_data['population'] = np.log(df_topredict_data['population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data, df_topredict_data = remove_unwanted_columns(df_training_data, df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data, df_topredict_data = small_n2(df_training_data, df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data, df_topredict_data = lda(df_training_data, df_topredict_data, df_training_labels, cols = ['gps_height', 'latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data, df_topredict_data = one_hot_encode(df_training_data, df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df_training_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df_training_data.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(criterion='gini',\n",
    "                                min_samples_split=6,\n",
    "                                n_estimators=1000,\n",
    "                                max_features='auto',\n",
    "                                oob_score=True,\n",
    "                                random_state=1,\n",
    "                                n_jobs=-1)\n",
    "                            \n",
    "rf.fit(df_training_data, df_training_labels.values.ravel())\n",
    "print (\"%.4f\" % rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = rf.predict(df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(predictions)\n",
    "ydf = pd.DataFrame(predictions)\n",
    "#ydf.head\n",
    "ydf1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_training_data, df_training_labels_num, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_consider = ['amount_tsh', 'gps_height', 'longitude', 'latitude', 'region_code', 'district_code', 'population', 'construction_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def run_random_forest_predictor(X_train1, y_train1, X_test1, y_test1, features_to_consider):\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "df1 = X_train[features_to_consider]\n",
    "rf.fit(df1, y_train)\n",
    "predictions = rf.predict(X_test[features_to_consider])\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
