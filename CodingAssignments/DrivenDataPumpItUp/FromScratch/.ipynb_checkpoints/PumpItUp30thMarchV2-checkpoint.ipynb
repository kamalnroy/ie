{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include required libs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_and_test_data():\n",
    "    train_features=pd.read_csv('TrainingSetValues.csv',parse_dates=True)\n",
    "    train_labels=pd.read_csv('TrainingSetLabels.csv')\n",
    "    to_predict_features=pd.read_csv('TestSetValues.csv',parse_dates=True)\n",
    "    \n",
    "    # merge training features and labels\n",
    "    #training_data = pd.merge(train_features, train_labels, how='inner', on=['id'])\n",
    "    \n",
    "    return train_features, train_labels, to_predict_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute median values for 0 amount_tsh\n",
    "def impute_missing_amount_tsh(df):\n",
    "    df.amount_tsh[df.amount_tsh <= 0] = np.median(df.amount_tsh[df.amount_tsh > 0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute median values for 0 gps height\n",
    "def impute_missing_gps_height(df):\n",
    "    df.gps_height[df.gps_height <= 0] = np.median(df.gps_height[df.gps_height > 0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute median values for 0 population\n",
    "def impute_missing_population(df):\n",
    "    df.population[df.population <= 0] = np.median(df.population[df.population > 0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform population into categories\n",
    "def label_population(row):\n",
    "    if row['population'] <=0:\n",
    "        return 'NA'\n",
    "    elif row['population'] >= 1 and row['population'] <= 40:\n",
    "        return 'A'\n",
    "    elif row['population'] >= 41 and row['population'] <= 67:\n",
    "        return 'B'\n",
    "    elif row['population'] >= 68 and row['population'] <= 99:\n",
    "        return 'C'\n",
    "    elif row['population'] >= 100 and row['population'] <= 131:\n",
    "        return 'D'\n",
    "    elif row['population'] >= 132 and row['population'] <= 175:\n",
    "        return 'E'\n",
    "    elif row['population'] >= 176 and row['population'] <= 219:\n",
    "        return 'F'\n",
    "    elif row['population'] >= 220 and row['population'] <= 259:\n",
    "        return 'G'\n",
    "    elif row['population'] >= 260 and row['population'] <= 349:\n",
    "        return 'H'\n",
    "    elif row['population'] >= 350 and row['population'] <= 448:\n",
    "        return 'I'\n",
    "    elif row['population'] >= 449 and row['population'] <= 598:\n",
    "        return 'J'\n",
    "    elif row['population'] >= 599 and row['population'] <= 1290:\n",
    "        return 'K'\n",
    "    elif row['population'] >= 1291:\n",
    "        return 'L'\n",
    "\n",
    "def transform_population_into_categories(df):\n",
    "    df['population_cat'] = df.apply(label_population, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute median values for construction year\n",
    "def impute_missing_construction_year(df):\n",
    "    df.construction_year[df.construction_year <= 0] = np.median(df.construction_year[df.construction_year > 0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute missing booleans with false and convert each value to float or integer\n",
    "def impute_missing_booleans(df, colname):\n",
    "    df[colname].fillna(False, inplace = True)\n",
    "    df[colname] = df[colname].apply(lambda x: float(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since random forest doesnt work on datetime, we will break them down to month and year.\n",
    "# But, also if we simply convert month into numerical values, it doesnt work well because \n",
    "# there may be big distance between Jan and December and also between 1970 to  2010, to take an example.\n",
    "# So its better to one hot encode them after transforming the date to month and year\n",
    "def transform_date_recorded_to_month_and_year(df):\n",
    "    df['date_recorded'] = pd.to_datetime(df['date_recorded'])\n",
    "    df['year_recorded'] = df['date_recorded'].apply(lambda x: x.year)\n",
    "    df['month_recorded'] = df['date_recorded'].apply(lambda x: x.month)\n",
    "    df['date_recorded'] = (pd.to_datetime(df['date_recorded'])).apply(lambda x: x.toordinal())\n",
    "    return df\n",
    "\n",
    "# One Hot encode year and month recorded (first convert the month and year to string beafore OHEing)\n",
    "# Also delete the original ones\n",
    "def ohe_month_and_year_recorded(df_train, df_test):\n",
    "    df_train = transform_date_recorded_to_month_and_year(df_train)\n",
    "    df_test = transform_date_recorded_to_month_and_year(df_test)\n",
    "    for col in ['month_recorded', 'year_recorded']:\n",
    "        df_train[col] = df_train[col].apply(lambda x: str(x))\n",
    "        df_test[col] = df_test[col].apply(lambda x: str(x))\n",
    "        ohe_cols_postfix = [col + '_' + i for i in df_train[col].unique() if i in df_test[col].unique()]\n",
    "        df_train = pd.concat((df_train, pd.get_dummies(df_train[col], prefix = col)[ohe_cols_postfix]), axis = 1)\n",
    "        df_test = pd.concat((df_test, pd.get_dummies(df_test[col], prefix = col)[ohe_cols_postfix]), axis = 1)\n",
    "        del df_test[col]\n",
    "        del df_train[col]\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def locs(X_train, X_test):\n",
    "    \"\"\"\n",
    "    fill in the nulls for ['longitude', 'latitude', 'gps_height', 'population'] by using means from \n",
    "    ['subvillage', 'district_code', 'basin'], and lastly the overall mean\n",
    "    \"\"\"\n",
    "    trans = ['longitude', 'latitude', 'gps_height', 'population']\n",
    "    for i in [X_train, X_test]:\n",
    "        i.loc[i.longitude == 0, 'latitude'] = 0\n",
    "    for z in trans:\n",
    "        for i in [X_train, X_test]:\n",
    "            i[z].replace(0., np.NaN, inplace = True)\n",
    "            i[z].replace(1., np.NaN, inplace = True)\n",
    "        \n",
    "        for j in ['subvillage', 'district_code', 'basin']:\n",
    "        \n",
    "            X_train['mean'] = X_train.groupby([j])[z].transform('mean')\n",
    "            X_train[z] = X_train[z].fillna(X_train['mean'])\n",
    "            o = X_train.groupby([j])[z].mean()\n",
    "            fill = pd.merge(X_test, pd.DataFrame(o), left_on=[j], right_index=True, how='left').iloc[:,-1]\n",
    "            X_test[z] = X_test[z].fillna(fill)\n",
    "        \n",
    "        X_train[z] = X_train[z].fillna(X_train[z].mean())\n",
    "        X_test[z] = X_test[z].fillna(X_train[z].mean())\n",
    "        del X_train['mean']\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to drop columns from a dataframe\n",
    "def drop_columns(df, cols_to_drop):\n",
    "    for col in cols_to_drop:\n",
    "        del df[col]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def gini(p):\n",
    "#    return 1-(p**2 + (1-p)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def small_n2(X_train, X_test):\n",
    "    cols = [i for i in X_train.columns if type(X_train[i].iloc[0]) == str]\n",
    "    X_train[cols] = X_train[cols].where(X_train[cols].apply(lambda x: x.map(x.value_counts())) > 100, \"other\")\n",
    "    for column in cols:\n",
    "        for i in X_test[column].unique():\n",
    "            if i not in X_train[column].unique():\n",
    "                X_test[column].replace(i, 'other', inplace=True)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda(X_train, X_test, y_train, cols=['population', 'gps_height', 'latitude', 'longitude']):\n",
    "    sc = StandardScaler()\n",
    "    X_train_std = sc.fit_transform(X_train[cols])\n",
    "    X_test_std = sc.transform(X_test[cols])\n",
    "    lda = LDA(n_components=None)\n",
    "    X_train_lda = lda.fit_transform(X_train_std, y_train.values.ravel())\n",
    "    X_test_lda = lda.transform(X_test_std)\n",
    "    X_train = pd.concat((pd.DataFrame(X_train_lda), X_train), axis=1)\n",
    "    X_test = pd.concat((pd.DataFrame(X_test_lda), X_test), axis=1)\n",
    "    for i in cols:\n",
    "        del X_train[i]\n",
    "        del X_test[i]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(df_train, df_test):\n",
    "    columns = [i for i in df_train.columns if type(df_train[i].iloc[0]) == str]\n",
    "    for column in columns:\n",
    "        df_train[column].fillna('NULL', inplace = True)\n",
    "        ohe_cols = [column+'_'+i for i in df_train[column].unique() if i in df_test[column].unique()]\n",
    "        df_train = pd.concat((df_train, pd.get_dummies(df_train[column], prefix = column)[ohe_cols]), axis = 1)\n",
    "        df_test = pd.concat((df_test, pd.get_dummies(df_test[column], prefix = column)[ohe_cols]), axis = 1)\n",
    "        del df_train[column]\n",
    "        del df_test[column]\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def one_hot_encode1(df):\n",
    "#    columns = [x for x in df.columns if type(df[x].iloc[0]) == str]\n",
    "#    for column in columns:\n",
    "#        df[column].fillna('NULL', inplace = True)\n",
    "#        ohe_cols = [column + '_' + x for x in df[column].unique() if x in df[column].unique()]\n",
    "#        df = pd.concat((df, pd.get_dummies(df[column], prefix = column)[ohe_cols]), axis = 1)\n",
    "#        del df[column]\n",
    "#    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function for column values analysis\n",
    "def analyze_unique_values_for_column(df, colname):\n",
    "    unique_col_vals = df[colname].unique()\n",
    "    tmp_str = \"Unique \" + colname + \"s:\"\n",
    "    print(\"****************************\")\n",
    "    print(tmp_str, unique_col_vals.size)\n",
    "    print(\"****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function for column values analysis\n",
    "def analyze_in_detail_unique_values_for_column(df, colname):\n",
    "    unique_col_vals = df[colname].unique()\n",
    "    tmp_str = \"Unique \" + colname + \"s:\"\n",
    "    print(\"****************************\")\n",
    "    print(tmp_str, unique_col_vals.size)\n",
    "    print(\"****************************\")\n",
    "    lessthan10 = 0\n",
    "    lessthan20 = 0\n",
    "    lessthan30 = 0\n",
    "    lessthan50 = 0\n",
    "    lessthan100 = 0\n",
    "    for val in unique_col_vals:\n",
    "        cnt = df[df[colname] == val][colname].count()\n",
    "        print(val, cnt) # uncomment this line if you want to see the count of each colname-value\n",
    "        if(cnt < 10):\n",
    "            lessthan10 +=1     \n",
    "            print(val, cnt)\n",
    "        elif(cnt < 20):\n",
    "            lessthan20 +=1\n",
    "        elif(cnt < 30):\n",
    "            lessthan30 +=1\n",
    "        elif(cnt < 50):\n",
    "            lessthan50 +=1\n",
    "\n",
    "    print(\"lessthan50: \", lessthan50 )\n",
    "    print(\"lessthan30: \", lessthan30 )\n",
    "    print(\"lessthan20: \", lessthan20 )\n",
    "    print(\"lessthan10: \", lessthan10 )\n",
    "    print(\"****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data, df_training_labels, df_topredict_data = load_train_and_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep a copy of this for the output file\n",
    "df_to_predict_data1 = df_topredict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Brief Stats of each column\n",
      "*****************************************\n",
      "                 id     amount_tsh    gps_height     longitude      latitude  \\\n",
      "count  59400.000000   59400.000000  59400.000000  59400.000000  5.940000e+04   \n",
      "mean   37115.131768     317.650385    668.297239     34.077427 -5.706033e+00   \n",
      "std    21453.128371    2997.574558    693.116350      6.567432  2.946019e+00   \n",
      "min        0.000000       0.000000    -90.000000      0.000000 -1.164944e+01   \n",
      "25%    18519.750000       0.000000      0.000000     33.090347 -8.540621e+00   \n",
      "50%    37061.500000       0.000000    369.000000     34.908743 -5.021597e+00   \n",
      "75%    55656.500000      20.000000   1319.250000     37.178387 -3.326156e+00   \n",
      "max    74247.000000  350000.000000   2770.000000     40.345193 -2.000000e-08   \n",
      "\n",
      "        num_private   region_code  district_code    population  \\\n",
      "count  59400.000000  59400.000000   59400.000000  59400.000000   \n",
      "mean       0.474141     15.297003       5.629747    179.909983   \n",
      "std       12.236230     17.587406       9.633649    471.482176   \n",
      "min        0.000000      1.000000       0.000000      0.000000   \n",
      "25%        0.000000      5.000000       2.000000      0.000000   \n",
      "50%        0.000000     12.000000       3.000000     25.000000   \n",
      "75%        0.000000     17.000000       5.000000    215.000000   \n",
      "max     1776.000000     99.000000      80.000000  30500.000000   \n",
      "\n",
      "       construction_year  \n",
      "count       59400.000000  \n",
      "mean         1300.652475  \n",
      "std           951.620547  \n",
      "min             0.000000  \n",
      "25%             0.000000  \n",
      "50%          1986.000000  \n",
      "75%          2004.000000  \n",
      "max          2013.000000  \n",
      "\n",
      "*****************************************\n",
      "number of nonzeros in each column\n",
      "*****************************************\n",
      "id                       59399\n",
      "amount_tsh               17761\n",
      "date_recorded            59400\n",
      "funder                   59400\n",
      "gps_height               38962\n",
      "installer                59400\n",
      "longitude                57588\n",
      "latitude                 59400\n",
      "wpt_name                 59400\n",
      "num_private                757\n",
      "basin                    59400\n",
      "subvillage               59400\n",
      "region                   59400\n",
      "region_code              59400\n",
      "district_code            59377\n",
      "lga                      59400\n",
      "ward                     59400\n",
      "population               38019\n",
      "public_meeting           54345\n",
      "recorded_by              59400\n",
      "scheme_management        59400\n",
      "scheme_name              59400\n",
      "permit                   41908\n",
      "construction_year        38691\n",
      "extraction_type          59400\n",
      "extraction_type_group    59400\n",
      "extraction_type_class    59400\n",
      "management               59400\n",
      "management_group         59400\n",
      "payment                  59400\n",
      "payment_type             59400\n",
      "water_quality            59400\n",
      "quality_group            59400\n",
      "quantity                 59400\n",
      "quantity_group           59400\n",
      "source                   59400\n",
      "source_type              59400\n",
      "source_class             59400\n",
      "waterpoint_type          59400\n",
      "waterpoint_type_group    59400\n",
      "dtype: int64\n",
      "\n",
      "*****************************************\n",
      "no. of nulls in each column\n",
      "*****************************************\n",
      "id                           0\n",
      "amount_tsh                   0\n",
      "date_recorded                0\n",
      "funder                    3635\n",
      "gps_height                   0\n",
      "installer                 3655\n",
      "longitude                    0\n",
      "latitude                     0\n",
      "wpt_name                     0\n",
      "num_private                  0\n",
      "basin                        0\n",
      "subvillage                 371\n",
      "region                       0\n",
      "region_code                  0\n",
      "district_code                0\n",
      "lga                          0\n",
      "ward                         0\n",
      "population                   0\n",
      "public_meeting            3334\n",
      "recorded_by                  0\n",
      "scheme_management         3877\n",
      "scheme_name              28166\n",
      "permit                    3056\n",
      "construction_year            0\n",
      "extraction_type              0\n",
      "extraction_type_group        0\n",
      "extraction_type_class        0\n",
      "management                   0\n",
      "management_group             0\n",
      "payment                      0\n",
      "payment_type                 0\n",
      "water_quality                0\n",
      "quality_group                0\n",
      "quantity                     0\n",
      "quantity_group               0\n",
      "source                       0\n",
      "source_type                  0\n",
      "source_class                 0\n",
      "waterpoint_type              0\n",
      "waterpoint_type_group        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"*****************************************\\nBrief Stats of each column\\n*****************************************\")\n",
    "print(df_training_data.describe())\n",
    "\n",
    "#getting number of nonzeros in each column\n",
    "print(\"\\n*****************************************\\nnumber of nonzeros in each column\\n*****************************************\")\n",
    "print(df_training_data.astype(bool).sum(axis=0))\n",
    "\n",
    "# getting no. of nulls in each column\n",
    "print(\"\\n*****************************************\\nno. of nulls in each column\\n*****************************************\")\n",
    "print(df_training_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#analyze_unique_values_for_column(df_training_data, \"funder\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"installer\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"wpt_name\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"basin\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"subvillage\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"region\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"region_code\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"district_code\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"lga\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"ward\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"recorded_by\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"scheme_management\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"scheme_name\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"extraction_type\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"extraction_type_group\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"extraction_type_class\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"management\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"management_group\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"management_group\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"payment\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"payment_type\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"management_group\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"water_quality\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"quality_group\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"quantity\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"quantity_group\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"source\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"source_type\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"source_class\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"source_class\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"waterpoint_type\")\n",
    "#analyze_unique_values_for_column(df_training_data, \"waterpoint_type_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#analyze_in_detail_unique_values_for_column(df_training_data, \"population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#analyze_in_detail_unique_values_for_column(df_training_data, \"management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#analyze_in_detail_unique_values_for_column(df_training_data, \"management_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#analyze_in_detail_unique_values_for_column(df_training_data, \"extraction_type_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_training_labels['id']\n",
    "\n",
    "#df_training_data, df_topredict_data = dates(df_training_data, df_topredict_data)\n",
    "#df_training_data = transform_date_recorded_to_month_and_year(df_training_data)\n",
    "#df_topredict_data = transform_date_recorded_to_month_and_year(df_topredict_data)\n",
    "\n",
    "df_training_data, df_topredict_data = ohe_month_and_year_recorded(df_training_data, df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# impute missing construction year with median construction year\n",
    "df_training_data = impute_missing_construction_year(df_training_data)\n",
    "df_topredict_data = impute_missing_construction_year(df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the fields public_meeting and permit are boolean, but there are many missing values \n",
    "# (3334 in public_meeting and 3056 in permit)\n",
    "# impute these missing values with FALSE\n",
    "df_training_data = impute_missing_booleans(df_training_data, \"public_meeting\")\n",
    "df_topredict_data = impute_missing_booleans(df_topredict_data, \"public_meeting\")\n",
    "\n",
    "df_training_data = impute_missing_booleans(df_training_data, \"permit\")\n",
    "df_topredict_data = impute_missing_booleans(df_topredict_data, \"permit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fill in the nulls for ['longitude', 'latitude', 'gps_height', 'population'] by using means from \n",
    "['subvillage', 'district_code', 'basin'], and lastly the overall mean\n",
    "\"\"\"\n",
    "df_training_data, df_topredict_data = locs(df_training_data, df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data['population'] = np.log(df_training_data['population'])\n",
    "df_topredict_data['population'] = np.log(df_topredict_data['population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'scheme_namewpt_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2524\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2525\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'scheme_namewpt_name'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b138248f0f2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;34m'waterpoint_type_group'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'extraction_type_group'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'recorded_by'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'subvillage'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'district_code'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lga'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ward'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'scheme_name'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m 'wpt_name', 'funder', 'installer']\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf_training_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrop_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_training_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns_to_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf_topredict_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrop_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_topredict_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns_to_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-fd0abde4c34c>\u001b[0m in \u001b[0;36mdrop_columns\u001b[1;34m(df, cols_to_drop)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdrop_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols_to_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols_to_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[1;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__delitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2094\u001b[0m             \u001b[1;31m# there was no match, this call should raise the appropriate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2095\u001b[0m             \u001b[1;31m# exception:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2096\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2098\u001b[0m         \u001b[1;31m# delete from the caches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mdelete\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   3900\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mselected\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnon\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3901\u001b[0m         \"\"\"\n\u001b[1;32m-> 3902\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3904\u001b[0m         \u001b[0mis_deleted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2525\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'scheme_namewpt_name'"
     ]
    }
   ],
   "source": [
    "# drop unwanted columns \n",
    "columns_to_drop = ['id','amount_tsh',  'num_private', 'region', 'quantity', 'quality_group', 'source_type', 'payment', \n",
    "'waterpoint_type_group', 'extraction_type_group', 'recorded_by', 'subvillage', 'district_code', 'lga', 'ward', 'scheme_name',\n",
    "'wpt_name', 'funder', 'installer']\n",
    "df_training_data = drop_columns(df_training_data, columns_to_drop)\n",
    "df_topredict_data = drop_columns(df_topredict_data, columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data, df_topredict_data = small_n2(df_training_data, df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data, df_topredict_data = lda(df_training_data, df_topredict_data, df_training_labels, cols = ['gps_height', 'latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encode the columns that have string values\n",
    "df_training_data,df_topredict_data = one_hot_encode(df_training_data, df_topredict_data)\n",
    "#df_topredict_data = one_hot_encode1(df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_training_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(df_training_data, df_training_labels, test_size = 0.25, random_state = 42)\n",
    "#    # Create random forest classifier instance\n",
    "## Train and Test dataset size details\n",
    "#print (\"X_train Shape :: \", X_train.shape)\n",
    "#print (\"y_train Shape :: \", y_train.shape)\n",
    "#print (\"X_test Shape :: \", X_test.shape)\n",
    "#print (\"y_test Shape :: \", y_test.shape)\n",
    "#\n",
    "#rf = RandomForestClassifier(criterion='gini', min_samples_split=6, n_estimators=1000, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n",
    "#rf.fit(X_train, y_train)\n",
    "#predictions = rf.predict(X_test)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for i in range(0, 5):\n",
    "##    print(y_test[i], predictions[i])    \n",
    "##    #print (\"Actual outcome :: {} and Predicted outcome :: {}\".format(list(y_test)[i], predictions[i]))\n",
    "#\n",
    "#print (\"Train Accuracy :: \", accuracy_score(y_train, rf.predict(X_train)))\n",
    "#print (\"Test Accuracy  :: \", accuracy_score(y_test, predictions))\n",
    "#print (\"Confusion matrix \", confusion_matrix(y_test, predictions))\n",
    "#\n",
    "#predictions = rf.predict(df_topredict_data)\n",
    "#df_predictions = pd.DataFrame(predictions)\n",
    "## Output the predictions into a csv file\n",
    "#columns = ['id','status_group']\n",
    "#df_submission = pd.DataFrame(columns=columns)\n",
    "##to_predict_features=pd.read_csv('TestSetValues.csv',parse_dates=True)\n",
    "#df_to_predict_data1 = df_to_predict_data1.reset_index(drop=True)\n",
    "#df_predictions = df_predictions.reset_index(drop=True)\n",
    "#df_submission = df_submission.reset_index(drop=True)\n",
    "#df_submission['id'] = df_to_predict_data1['id']\n",
    "#df_submission['status_group'] = df_predictions[0]\n",
    "#df_submission.to_csv(\"submission_ab_kn.csv\", sep=\",\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(criterion='gini', min_samples_split=6, n_estimators=1000, max_features='auto',\n",
    "     oob_score=True, random_state=1, n_jobs=-1)\n",
    "rf.fit(df_training_data, df_training_labels.values.ravel())\n",
    "print (\"%.4f\" % rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = rf.predict(df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output the predictions into a csv file\n",
    "columns = ['id','status_group']\n",
    "df_submission = pd.DataFrame(columns=columns)\n",
    "#to_predict_features=pd.read_csv('TestSetValues.csv',parse_dates=True)\n",
    "df_to_predict_data1 = df_to_predict_data1.reset_index(drop=True)\n",
    "df_predictions = df_predictions.reset_index(drop=True)\n",
    "df_submission = df_submission.reset_index(drop=True)\n",
    "df_submission['id'] = df_to_predict_data1['id']\n",
    "df_submission['status_group'] = df_predictions[0]\n",
    "df_submission.to_csv(\"submission_ab_kn.csv\", sep=\",\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_submission.shape)\n",
    "print(df_submission.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert string labels to numerics\n",
    "#label_map = {\"functional\": 1, \"functional needs repair\": 2, \"non functional\": 3}\n",
    "#df_training_data['status_group_num']= df_training_data['status_group'].map(label_map).astype(int)\n",
    "\n",
    "# do sanity check\n",
    "#df_training_data[['id', 'amount_tsh', 'status_group', 'status_group_num']].head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the training labels into a df\n",
    "#df_training_labels_str = df_training_data['status_group']\n",
    "#df_training_labels_num = np.array(df_training_data['status_group_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop the training labels from the training set\n",
    "#df_training_data= df_training_data.drop('status_group', axis = 1)\n",
    "#df_training_data= df_training_data.drop('status_group_num', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def one_hot_encode(X_train, X_test):\n",
    "#    columns = [i for i in X_train.columns if type(X_train[i].iloc[0]) == str]\n",
    "#    for column in columns:\n",
    "#        X_train[column].fillna('NULL', inplace = True)\n",
    "#        good_cols = [column+'_'+i for i in X_train[column].unique() if i in X_test[column].unique()]\n",
    "#        X_train = pd.concat((X_train, pd.get_dummies(X_train[column], prefix = column)[good_cols]), axis = 1)\n",
    "#        X_test = pd.concat((X_test, pd.get_dummies(X_test[column], prefix = column)[good_cols]), axis = 1)\n",
    "#        del X_train[column]\n",
    "#        del X_test[column]\n",
    "#    return X_train, X_test\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute missing population \n",
    "#df_training_data = impute_missing_population1(df_training_data)\n",
    "#df_topredict_data = impute_missing_population1(df_topredict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#analyze_in_detail_unique_values_for_column(df_training_data, 'population_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "#X_train, X_test, y_train, y_test = \n",
    "#    train_test_split(df_training_data, df_training_labels_num, test_size = 0.25, random_state = 42)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "#features_to_consider =\n",
    "#    ['amount_tsh', 'gps_height', 'longitude', 'latitude', 'region_code', 'district_code', 'population', 'construction_year']\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "##def run_random_forest_predictor(X_train1, y_train1, X_test1, y_test1, features_to_consider):\n",
    "#rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "## Train the model on training data\n",
    "#df1 = X_train[features_to_consider]\n",
    "#rf.fit(df1, y_train)\n",
    "#predictions = rf.predict(X_test[features_to_consider])\n",
    "## Calculate the absolute errors\n",
    "#errors = abs(predictions - y_test)\n",
    "## Print out the mean absolute error (mae)\n",
    "#print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
