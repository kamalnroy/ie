{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Andrea\n",
      "[nltk_data]     Blasioli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e26dc6a02770>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#from tensorflow import keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import decomposition, ensemble\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "nltk.download()\n",
    "nltk.download('wordnet')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import scipy.sparse as sp\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "#from tensorflow import keras\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_training = pd.read_csv(\"fake_or_real_news_training.csv\")\n",
    "data_training = data_training[(data_training[\"label\"] == \"FAKE\") | (data_training[\"label\"] == \"REAL\")]\n",
    "data_testing = pd.read_csv(\"fake_or_real_news_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Shape--\n",
      "(3966, 6)\n",
      "--Head--\n",
      "      ID                                              title  \\\n",
      "0   8476                       You Can Smell Hillary’s Fear   \n",
      "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4    875   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label   X1   X2  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  NaN  NaN  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  NaN  NaN  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  NaN  NaN  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  NaN  NaN  \n",
      "4  It's primary day in New York and front-runners...  REAL  NaN  NaN  \n",
      "--Class Balance--\n",
      "REAL    1990\n",
      "FAKE    1976\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "overview = True\n",
    "if overview:\n",
    "    print(\"--Shape--\")\n",
    "    print(data_training.shape)\n",
    "    print(\"--Head--\")\n",
    "    print(data_training.head(5))\n",
    "    print(\"--Class Balance--\")\n",
    "    print(data_training[\"label\"].value_counts())\n",
    "else:\n",
    "    print(data_training.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - NLP Code and ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a- Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagofwords(df, variable, step, vectorizer = None, technique=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Apply the standard technics to compute bag of words from the corpus \n",
    "    \"\"\"\n",
    "    if step == \"training\":\n",
    "        if technique == \"basic\":\n",
    "            vect = CountVectorizer(stop_words=\"english\")\n",
    "            train_counts = vect.fit_transform(df[variable].values)\n",
    "        elif technique == \"tfidf\":\n",
    "            vect = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "            train_counts = vect.fit_transform(df[variable].values)\n",
    "        else:\n",
    "            train_counts = None\n",
    "            print(\"Choose valid technique\")\n",
    "        if verbose:\n",
    "            print(\"--\" + variable + \"--\")\n",
    "            print(train_counts.shape)\n",
    "            print(\"--Sample Features--\")\n",
    "            print(random.sample(vect.get_feature_names(), 50))\n",
    "        return train_counts, vect\n",
    "    else:\n",
    "        train_counts = vectorizer.transform(df[variable].values)\n",
    "        return train_counts\n",
    "\n",
    "def train_test(df, threshold, verbose=False):\n",
    "    \"\"\"\n",
    "    Split the dataset between training and testing\n",
    "    \"\"\"\n",
    "    tr, te = train_test_split(df, test_size = threshold, random_state = 42)\n",
    "    if verbose:\n",
    "        print(tr.shape)\n",
    "        print(te.shape)\n",
    "        print(tr.head(5))\n",
    "    return tr, te\n",
    "\n",
    "def use_title_and_text(df,):\n",
    "    \"\"\"\n",
    "    Combine the information of the title and text variable. Apply the prefix title if the word comes from \n",
    "    the title variable in order to differentiate the source of the information. \n",
    "    \"\"\"\n",
    "    titles = np.array([ \" \".join([\"title\" + \"_\" + y for y in x.split(\" \")]) for x in df[\"title\"].values ])\n",
    "    texts = np.array([ \": \" + x for x in df[\"text\"].values])\n",
    "    df[\"text_and_title\"] = np.core.defchararray.add(titles, texts)\n",
    "    return df\n",
    "\n",
    "def stem_lem(element):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    element_lemmatized = \" \".join([lmtzr.lemmatize(x) for x in element.split(\" \")])\n",
    "    return element_lemmatized\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b - Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "techniques_to_use = [\"tfidf\", \"basic\"][0]\n",
    "use_title = True\n",
    "lemmatize = True\n",
    "engineered_features = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if engineered_features:\n",
    "    for df in [data_training, data_testing]:\n",
    "        df['char_count_text'] = df['text'].apply(len)\n",
    "        df['char_count_title'] = df['title'].apply(len)\n",
    "        df['word_count_text'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['word_count_title'] = df['title'].apply(lambda x: len(x.split()))\n",
    "        df['word_density_text'] = df['char_count_text'] / (df['word_count_text']+1)\n",
    "        df['word_density_title'] = df['char_count_title'] / (df['word_count_title']+1)\n",
    "        df['upper_case_word_count_text'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "        df['upper_case_word_count_title'] = df['title'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "        if lemmatize:\n",
    "            df['title'] = df[\"title\"].apply(stem_lem)\n",
    "            df['text'] = df[\"text\"].apply(stem_lem)\n",
    "\n",
    "if use_title:\n",
    "    text_var = [\"text\", \"text_and_title\"][1]\n",
    "    data_training = use_title_and_text(data_training)\n",
    "    data_testing = use_title_and_text(data_testing)\n",
    "else:\n",
    "    text_var = [\"text\", \"text_and_title\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d - Define Test and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test(data_training, 0.2)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e - Execution BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--text_and_title--\n",
      "(3172, 56747)\n",
      "--Sample Features--\n",
      "['tarot', 'requests', 'darn', 'leone', 'title_u', 'censored', 'shakier', 'consequent', 'title_deaths', 'portly', 'shooter', 'walkouts', 'rarely', 'title_pot', 'title_repeatedly', 'title_wundergroundmusic', 'selling', 'blacks', 'scraper', 'qtaiba', 'hatchback', 'typing', 'mace', 'rye', 'malignant', 'forehead', 'divulge', 'adivasis', 'breed', 'elicits', 'title_affiliate', 'title_suitcase', 'wolverine', 'title_krishnan', 'innovations', 'barr', 'session', 'stretches', 'congresses', 'posters', 'theblogmire', 'title_end', 'dwy2rdwyho', 'decellularization', 'painfully', 'johnetta', 'temperatures', 'dubin', 'toughen', 'existing']\n"
     ]
    }
   ],
   "source": [
    "features_train, c_vec_train = bagofwords(train, text_var, \"training\", technique=techniques_to_use, verbose=True)\n",
    "features_test = bagofwords(test, text_var, \"testing\", vectorizer=c_vec_train)\n",
    "final_features_train, c_vec_train_final = bagofwords(data_training, text_var, \"training\", technique=techniques_to_use)\n",
    "final_features_test = bagofwords(data_testing, text_var, \"testing\", vectorizer=c_vec_train_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f - Execution Code Test and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_title:\n",
    "    df_features_train = train.join(pd.DataFrame(features_train.todense()).reset_index(drop=True), how='inner').drop([\"ID\", \"title\", \"text\", \"text_and_title\", \"X1\", \"X2\", \"label\"], axis=1)\n",
    "    df_features_test = test.join(pd.DataFrame(features_test.todense()).reset_index(drop=True), how='inner').drop([\"ID\", \"title\", \"text\", \"text_and_title\", \"label\", \"X1\", \"X2\"], axis=1)\n",
    "    df_final_features_train = data_training.join(pd.DataFrame(final_features_train.todense()).reset_index(drop=True), how='inner').drop([\"ID\", \"title\", \"text\", \"text_and_title\", \"X1\", \"X2\"], axis=1)\n",
    "    df_final_features_test = data_testing.join(pd.DataFrame(final_features_test.todense()).reset_index(drop=True), how='inner').drop([\"ID\", \"title\", \"text\", \"text_and_title\"], axis=1)\n",
    "else: \n",
    "    df_features_train = train.join(pd.DataFrame(features_train.todense()).reset_index(drop=True), how='inner').drop([\"ID\", \"title\", \"text\", \"X1\", \"X2\", \"label\"], axis=1)\n",
    "    df_features_test = test.join(pd.DataFrame(features_test.todense()).reset_index(drop=True), how='inner').drop([\"ID\", \"title\", \"text\", \"label\", \"X1\", \"X2\"], axis=1)\n",
    "    df_final_features_train = data_training.join(pd.DataFrame(final_features_train.todense()).reset_index(drop=True), how='inner').drop([\"ID\", \"title\", \"text\", \"X1\", \"X2\"], axis=1)\n",
    "    df_final_features_test = data_testing.join(pd.DataFrame(final_features_test.todense()).reset_index(drop=True), how='inner').drop([\"ID\", \"title\", \"text\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# g - Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained = True\n",
    "if not trained:\n",
    "    print(\"--Training--\")\n",
    "    clf = MultinomialNB().fit(preprocessing.normalize(df_features_train.values), train[\"label\"])\n",
    "    sgd = SGDClassifier().fit(preprocessing.normalize(df_features_train.values), train[\"label\"])\n",
    "    rf = RandomForestClassifier(n_estimators=200).fit(df_features_train, train[\"label\"])\n",
    "    ada = AdaBoostClassifier(n_estimators=200).fit(df_features_train, train[\"label\"])\n",
    "    pickle.dump([clf, sgd, rf, ada], open(\"models_trained.pkl\", 'wb'))\n",
    "else:\n",
    "    models = pickle.load(open(\"models_trained.pkl\", 'rb'))\n",
    "    clf = models[0]\n",
    "    sgd = models[1]\n",
    "    rf = models[2]\n",
    "    ada = models[3]\n",
    "    for model in models:\n",
    "        print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f - Print Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-c113636e48c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredicted_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_features_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpredicted_sgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_features_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpredicted_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_features_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpredicted_ada\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mada\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_features_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "predicted_clf = clf.predict(df_features_test)\n",
    "predicted_sgd = sgd.predict(df_features_test)\n",
    "predicted_rf = rf.predict(df_features_test)\n",
    "predicted_ada = ada.predict(df_features_test)\n",
    "\n",
    "print(\"Naive Classifier Accuracy: {0}\".format(metrics.accuracy_score(predicted_clf, test[\"label\"])))\n",
    "print(\"SVM Accuracy: {0}\".format(metrics.accuracy_score(predicted_sgd, test[\"label\"])))\n",
    "print(\"Random Forest Accuracy: {0}\".format(metrics.accuracy_score(predicted_rf, test[\"label\"])))\n",
    "print(\"AdaBoost Accuracy: {0}\".format(metrics.accuracy_score(predicted_ada, test[\"label\"])))\n",
    "\n",
    "print(\"--Confusion Matrix Naive--\")\n",
    "print(metrics.confusion_matrix(predicted_clf, test[\"label\"]))\n",
    "print(\"--Confusion Matrix SVM--\")\n",
    "print(metrics.confusion_matrix(predicted_sgd, test[\"label\"]))\n",
    "print(\"--Confusion Matrix RF--\")\n",
    "print(metrics.confusion_matrix(predicted_rf, test[\"label\"]))\n",
    "print(\"--Confusion Matrix ADA--\")\n",
    "print(metrics.confusion_matrix(predicted_ada, test[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - FAKE VS REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sgd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a341de479ec6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclass_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_vec_train_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfeat_with_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"---\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"---\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat_with_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sgd' is not defined"
     ]
    }
   ],
   "source": [
    "class_labels = sgd.classes_\n",
    "feature_names = c_vec_train_final.get_feature_names()\n",
    "feat_with_weights = sorted(zip(sgd.coef_[0], feature_names))\n",
    "print(\"---\" + class_labels[0] + \"---\")\n",
    "print(feat_with_weights[:10])\n",
    "print(\"---\" + class_labels[1] + \"---\")\n",
    "print(feat_with_weights[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_trained = True\n",
    "if not final_trained:\n",
    "    model = SGDClassifier().fit(preprocessing.normalize(df_final_features_train.drop(\"label\", axis=1).values), df_final_features_train[\"label\"])\n",
    "    pickle.dump(model, open(\"final_model.pkl\", 'wb'))\n",
    "else:\n",
    "    model = pickle.load(open(\"final_model.pkl\", 'rb'))\n",
    "    print(model)\n",
    "predicted_sgd = model.predict(preprocessing.normalize(df_final_features_test.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_data = {\"IDs\": range(0, len(predicted_sgd)), \"Predictions\": predicted_sgd}\n",
    "df_predictions = pd.DataFrame(data=pred_data)\n",
    "df_predictions.to_csv(\"predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
