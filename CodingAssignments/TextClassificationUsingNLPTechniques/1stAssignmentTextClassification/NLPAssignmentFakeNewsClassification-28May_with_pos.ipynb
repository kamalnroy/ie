{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Following function cleans stop words and/or punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_puncts_and_stop_words(text):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    #return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    #nostopwords = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    #nostopwords = ' '.join(nostopwords)\n",
    "    #print(nostopwords)\n",
    "    return nostopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    #txt = \"\"\"Resumption of the session I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .\"\"\"\n",
    "    lemmatized_text = ([wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(text))])\n",
    "    lemmatized_text = ' '.join(lemmatized_text)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_pos_tagging(text):\n",
    "    #print(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    #print(\"********************\")\n",
    "    #print(tokens)\n",
    "    #print(\"*********************\")\n",
    "    tokens = nltk.pos_tag(tokens)\n",
    "    #print(tokens)\n",
    "    tokens = [t[0]+'_'+ t[1] for t in tokens]\n",
    "    pos_tagged_text = ' '.join(tokens)\n",
    "    return pos_tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"fake_or_real_news_training.csv\")\n",
    "test_df = pd.read_csv(\"fake_or_real_news_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    df['cleaned_text'] = df['text'].apply(remove_puncts_and_stop_words)\n",
    "    df.drop('text', axis=1, inplace=True)\n",
    "    df.rename(columns={'cleaned_text': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lemmatize_text(train_df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tagged = do_pos_tagging(train_df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_df['text'] = train_df[\"text\"].apply(lemmatize_text)\n",
    "#test_df['text'] = test_df[\"text\"].apply(lemmatize_text)\n",
    "train_df['text'] = train_df[\"text\"].apply(do_pos_tagging)\n",
    "test_df['text'] = test_df[\"text\"].apply(do_pos_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel_NNP Greenfield_NNP ,_, a_DT Shillman_NN...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google_NNP Pinterest_NNP Digg_NNP Linkedin_NNP...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S._NNP Secretary_NNP of_IN State_NNP John_NN...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>—_JJ Kaydee_NNP King_NNP (_( @_NNP KaydeeKing_...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It_PRP 's_VBZ primary_JJ day_NN in_IN New_NNP ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label   X1   X2  \n",
       "0  Daniel_NNP Greenfield_NNP ,_, a_DT Shillman_NN...  FAKE  NaN  NaN  \n",
       "1  Google_NNP Pinterest_NNP Digg_NNP Linkedin_NNP...  FAKE  NaN  NaN  \n",
       "2  U.S._NNP Secretary_NNP of_IN State_NNP John_NN...  REAL  NaN  NaN  \n",
       "3  —_JJ Kaydee_NNP King_NNP (_( @_NNP KaydeeKing_...  FAKE  NaN  NaN  \n",
       "4  It_PRP 's_VBZ primary_JJ day_NN in_IN New_NNP ...  REAL  NaN  NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df['text'], train_df['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "#encoder = preprocessing.LabelEncoder()\n",
    "#train_y = encoder.fit_transform(train_y)\n",
    "#valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_labels(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    #predictions = predict_labels(classifier, feature_vector_valid, is_neural_net)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_predictions_to_csv(df_test, df_predictions, filename):\n",
    "    # Output the predictions into a csv file\n",
    "    columns = ['ID','label']\n",
    "    df_submission = pd.DataFrame(columns=columns)\n",
    "    #to_predict_features=pd.read_csv('TestSetValues.csv',parse_dates=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    df_predictions = df_predictions.reset_index(drop=True)\n",
    "    df_submission = df_submission.reset_index(drop=True)\n",
    "    df_submission['ID'] = df_test['ID']\n",
    "    df_submission['label'] = df_predictions[0]\n",
    "    df_submission.to_csv(filename, sep=\",\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.89\n",
      "NB, WordLevel TF-IDF:  0.891\n",
      "NB, N-Gram Vectors:  0.867\n",
      "NB, CharLevel Vectors:  0.842\n"
     ]
    }
   ],
   "source": [
    "def predict_using_CountVectorizer(to_predict_df):\n",
    "    # create a count vectorizer object \n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words='english')\n",
    "    #count_vect = CountVectorizer(analyzer='text_process')\n",
    "    #count_vect.fit(train_df['text'])\n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain_count =  count_vect.fit_transform(train_x)\n",
    "    xvalid_count =  count_vect.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Count Vectors\n",
    "    accuracy = train_model(MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "    print (\"NB, Count Vectors: \", accuracy)\n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_count = count_vect.transform(test_x)\n",
    "    predictions = predict_labels(MultinomialNB(), xtrain_count, train_y, xtest_count)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"CountVectorizer.prediction.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_WordLevelTfidfVectorizer(to_predict_df):\n",
    "    # create a count vectorizer object \n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "    #tfidf_vect.fit(train_df['text'])\n",
    "    \n",
    "    xtrain_tfidf =  tfidf_vect.fit_transform(train_x)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "    # Naive Bayes on Word Level TF IDF Vectors\n",
    "    accuracy = train_model(MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "    print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf = tfidf_vect.transform(test_x)\n",
    "    predictions = predict_labels(MultinomialNB(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"WordLevelTfidfVectorizer.prediction.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_NGramLevelTfidfVectorizer(to_predict_df):\n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    #tfidf_vect_ngram.fit(train_df['text'])\n",
    "    \n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.fit_transform(train_x)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print (\"NB, N-Gram Vectors: \", accuracy)  \n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "    predictions = predict_labels(MultinomialNB(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"NGramLevelTfidfVectorizer.prediction.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_CharLevelTfidfVectorizer(to_predict_df):\n",
    "    # characters level tf-idf\n",
    "    tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    #tfidf_vect_ngram_chars.fit(train_df['text'])\n",
    "    \n",
    "    xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.fit_transform(train_x) \n",
    "    xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "    # Naive Bayes on Character Level TF IDF Vectors\n",
    "    accuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "    print (\"NB, CharLevel Vectors: \", accuracy)   \n",
    "\n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_vect_ngram_chars = tfidf_vect_ngram_chars.transform(test_x)\n",
    "    predictions = predict_labels(MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_vect_ngram_chars)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"CharLevelTfidfVectorizer.prediction.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "\n",
    "def predict_using_naive_bayes():\n",
    "    predictions = predict_using_CountVectorizer(test_df)\n",
    "    \n",
    "    predictions = predict_using_WordLevelTfidfVectorizer(test_df)\n",
    "    \n",
    "    predictions = predict_using_NGramLevelTfidfVectorizer(test_df)\n",
    "    \n",
    "    predictions = predict_using_CharLevelTfidfVectorizer(test_df)\n",
    "    \n",
    "predict_using_naive_bayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using Logistic Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.915\n",
      "LR, WordLevel TF-IDF:  0.889\n",
      "LR, N-Gram Vectors:  0.892\n",
      "LR, CharLevel Vectors:  0.851\n"
     ]
    }
   ],
   "source": [
    "def predict_using_CountVectorizer_logit(to_predict_df):\n",
    "    # create a count vectorizer object \n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words='english')\n",
    "    #count_vect.fit(train_df['text'])\n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain_count =  count_vect.fit_transform(train_x)\n",
    "    xvalid_count =  count_vect.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Count Vectors\n",
    "    accuracy = train_model(LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "    print (\"LR, Count Vectors: \", accuracy)\n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_count = count_vect.transform(test_x)\n",
    "    predictions = predict_labels(LogisticRegression(), xtrain_count, train_y, xtest_count)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"CountVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_WordLevelTfidfVectorizer_logit(to_predict_df):\n",
    "    # create a count vectorizer object \n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "    #tfidf_vect.fit(train_df['text'])\n",
    "    \n",
    "    xtrain_tfidf =  tfidf_vect.fit_transform(train_x)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "    # Naive Bayes on Word Level TF IDF Vectors\n",
    "    accuracy = train_model(LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "    print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf = tfidf_vect.transform(test_x)\n",
    "    predictions = predict_labels(LogisticRegression(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"WordLevelTfidfVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_NGramLevelTfidfVectorizer_logit(to_predict_df):\n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    #tfidf_vect_ngram.fit(train_df['text'])\n",
    "    \n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.fit_transform(train_x)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print (\"LR, N-Gram Vectors: \", accuracy)  \n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "    predictions = predict_labels(LogisticRegression(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"NGramLevelTfidfVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_CharLevelTfidfVectorizer_logit(to_predict_df):\n",
    "    # characters level tf-idf\n",
    "    tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    #tfidf_vect_ngram_chars.fit(train_df['text'])\n",
    "    \n",
    "    xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.fit_transform(train_x) \n",
    "    xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "    # Naive Bayes on Character Level TF IDF Vectors\n",
    "    accuracy = train_model(LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "    print (\"LR, CharLevel Vectors: \", accuracy)   \n",
    "\n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_vect_ngram_chars = tfidf_vect_ngram_chars.transform(test_x)\n",
    "    predictions = predict_labels(LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_vect_ngram_chars)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"CharLevelTfidfVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_logistic():\n",
    "    predictions = predict_using_CountVectorizer_logit(test_df)\n",
    "    \n",
    "    predictions = predict_using_WordLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "    predictions = predict_using_NGramLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "    predictions = predict_using_CharLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "predict_using_logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.927\n"
     ]
    }
   ],
   "source": [
    "def predict_using_NGramLevelTfidfVectorizer_svm(to_predict_df):\n",
    "    #tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    tfidf_vect_ngram = TfidfVectorizer(stop_words = stop_words)\n",
    "    #tfidf_vect_ngram.fit(train_df['text'])\n",
    "    \n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.fit_transform(train_x)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(svm.LinearSVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print (\"SVM, N-Gram Vectors: \", accuracy)  \n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "    predictions = predict_labels(svm.LinearSVC(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"NGramLevelTfidfVectorizer.prediction.svm.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_svm():\n",
    "    predictions = predict_using_NGramLevelTfidfVectorizer_svm(test_df)\n",
    "    \n",
    "    #predictions = predict_using_WordLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "    #predictions = predict_using_NGramLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "    #predictions = predict_using_CharLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "predict_using_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## word level tf-idf\n",
    "#tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "#tfidf_vect.fit(train_df['text'])\n",
    "#xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "#xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "#\n",
    "## Naive Bayes on Word Level TF IDF Vectors\n",
    "#accuracy = train_model(MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "#print (\"NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ngram level tf-idf \n",
    "#tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "#tfidf_vect_ngram.fit(train_df['text'])\n",
    "#xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "#xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "#\n",
    "## Naive Bayes on Ngram Level TF IDF Vectors\n",
    "#accuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "#print (\"NB, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## characters level tf-idf\n",
    "#tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "#tfidf_vect_ngram_chars.fit(train_df['text'])\n",
    "#xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "#xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "#\n",
    "## Naive Bayes on Character Level TF IDF Vectors\n",
    "#accuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "#print (\"NB, CharLevel Vectors: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
