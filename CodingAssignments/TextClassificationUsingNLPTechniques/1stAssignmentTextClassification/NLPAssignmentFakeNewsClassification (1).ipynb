{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Following function cleans stop words and/or punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(example_sent):\n",
    " \n",
    "    stop_words = set(stopwords.words('english'))\n",
    " \n",
    "    word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    " \n",
    "    filtered_sentence = []\n",
    " \n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    " \n",
    "    example_sent = ' '.join(filtered_sentence[0:])\n",
    "    \n",
    "    \n",
    "    #print(word_tokens)\n",
    "    print(filtered_sentence)\n",
    "    print(example_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"fake_or_real_news_training.csv\")\n",
    "test_df = pd.read_csv(\"fake_or_real_news_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[0:5]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'].head(5).apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_title_text = train_df['title'] + \". \" + train_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_title_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=.33)\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df['text'], train_df['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "#encoder = preprocessing.LabelEncoder()\n",
    "#train_y = encoder.fit_transform(train_y)\n",
    "#valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_labels(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    #predictions = predict_labels(classifier, feature_vector_valid, is_neural_net)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_predictions_to_csv(df_test, df_predictions, filename):\n",
    "    # Output the predictions into a csv file\n",
    "    columns = ['ID','label']\n",
    "    df_submission = pd.DataFrame(columns=columns)\n",
    "    #to_predict_features=pd.read_csv('TestSetValues.csv',parse_dates=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    df_predictions = df_predictions.reset_index(drop=True)\n",
    "    df_submission = df_submission.reset_index(drop=True)\n",
    "    df_submission['ID'] = df_test['ID']\n",
    "    df_submission['label'] = df_predictions[0]\n",
    "    df_submission.to_csv(filename, sep=\",\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_CountVectorizer(to_predict_df):\n",
    "    # create a count vectorizer object \n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words='english')\n",
    "    #count_vect = CountVectorizer(analyzer='text_process')\n",
    "    count_vect.fit(train_df['text'])\n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain_count =  count_vect.transform(train_x)\n",
    "    xvalid_count =  count_vect.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Count Vectors\n",
    "    accuracy = train_model(MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "    print (\"NB, Count Vectors: \", accuracy)\n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_count = count_vect.transform(test_x)\n",
    "    predictions = predict_labels(MultinomialNB(), xtrain_count, train_y, xtest_count)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"CountVectorizer.prediction.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_WordLevelTfidfVectorizer(to_predict_df):\n",
    "    # create a count vectorizer object \n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "    tfidf_vect.fit(train_df['text'])\n",
    "    \n",
    "    xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "    # Naive Bayes on Word Level TF IDF Vectors\n",
    "    accuracy = train_model(MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "    print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf = tfidf_vect.transform(test_x)\n",
    "    predictions = predict_labels(MultinomialNB(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"WordLevelTfidfVectorizer.prediction.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_NGramLevelTfidfVectorizer(to_predict_df):\n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    tfidf_vect_ngram.fit(train_df['text'])\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print (\"NB, N-Gram Vectors: \", accuracy)  \n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "    predictions = predict_labels(MultinomialNB(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"NGramLevelTfidfVectorizer.prediction.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_CharLevelTfidfVectorizer(to_predict_df):\n",
    "    # characters level tf-idf\n",
    "    tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    tfidf_vect_ngram_chars.fit(train_df['text'])\n",
    "    xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "    xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "    # Naive Bayes on Character Level TF IDF Vectors\n",
    "    accuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "    print (\"NB, CharLevel Vectors: \", accuracy)   \n",
    "\n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_vect_ngram_chars = tfidf_vect_ngram_chars.transform(test_x)\n",
    "    predictions = predict_labels(MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_vect_ngram_chars)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"CharLevelTfidfVectorizer.prediction.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "\n",
    "def predict_using_naive_bayes():\n",
    "    predictions = predict_using_CountVectorizer(test_df)\n",
    "    \n",
    "    predictions = predict_using_WordLevelTfidfVectorizer(test_df)\n",
    "    \n",
    "    predictions = predict_using_NGramLevelTfidfVectorizer(test_df)\n",
    "    \n",
    "    predictions = predict_using_CharLevelTfidfVectorizer(test_df)\n",
    "    \n",
    "predict_using_naive_bayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using Logistic Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_CountVectorizer_logit(to_predict_df):\n",
    "    # create a count vectorizer object \n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words='english')\n",
    "    count_vect.fit(train_df['text'])\n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain_count =  count_vect.transform(train_x)\n",
    "    xvalid_count =  count_vect.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Count Vectors\n",
    "    accuracy = train_model(LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "    print (\"LR, Count Vectors: \", accuracy)\n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_count = count_vect.transform(test_x)\n",
    "    predictions = predict_labels(LogisticRegression(), xtrain_count, train_y, xtest_count)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"CountVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_WordLevelTfidfVectorizer_logit(to_predict_df):\n",
    "    # create a count vectorizer object \n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "    tfidf_vect.fit(train_df['text'])\n",
    "    \n",
    "    xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "    # Naive Bayes on Word Level TF IDF Vectors\n",
    "    accuracy = train_model(LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "    print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf = tfidf_vect.transform(test_x)\n",
    "    predictions = predict_labels(LogisticRegression(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"WordLevelTfidfVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_NGramLevelTfidfVectorizer_logit(to_predict_df):\n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    tfidf_vect_ngram.fit(train_df['text'])\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print (\"LR, N-Gram Vectors: \", accuracy)  \n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "    predictions = predict_labels(LogisticRegression(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"NGramLevelTfidfVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_CharLevelTfidfVectorizer_logit(to_predict_df):\n",
    "    # characters level tf-idf\n",
    "    tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    tfidf_vect_ngram_chars.fit(train_df['text'])\n",
    "    xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "    xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "    # Naive Bayes on Character Level TF IDF Vectors\n",
    "    accuracy = train_model(LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "    print (\"LR, CharLevel Vectors: \", accuracy)   \n",
    "\n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_vect_ngram_chars = tfidf_vect_ngram_chars.transform(test_x)\n",
    "    predictions = predict_labels(LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_vect_ngram_chars)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"CharLevelTfidfVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_logistic():\n",
    "    predictions = predict_using_CountVectorizer_logit(test_df)\n",
    "    \n",
    "    predictions = predict_using_WordLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "    predictions = predict_using_NGramLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "    predictions = predict_using_CharLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "predict_using_logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_NGramLevelTfidfVectorizer_svm(to_predict_df):\n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    tfidf_vect_ngram.fit(train_df['text'])\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print (\"LR, N-Gram Vectors: \", accuracy)  \n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "    predictions = predict_labels(svm.SVC(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"NGramLevelTfidfVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_svm():\n",
    "    predictions = predict_using_NGramLevelTfidfVectorizer_svm(test_df)\n",
    "    \n",
    "    #predictions = predict_using_WordLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "    #predictions = predict_using_NGramLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "    #predictions = predict_using_CharLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "predict_using_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## word level tf-idf\n",
    "#tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "#tfidf_vect.fit(train_df['text'])\n",
    "#xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "#xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "#\n",
    "## Naive Bayes on Word Level TF IDF Vectors\n",
    "#accuracy = train_model(MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "#print (\"NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ngram level tf-idf \n",
    "#tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "#tfidf_vect_ngram.fit(train_df['text'])\n",
    "#xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "#xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "#\n",
    "## Naive Bayes on Ngram Level TF IDF Vectors\n",
    "#accuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "#print (\"NB, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## characters level tf-idf\n",
    "#tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "#tfidf_vect_ngram_chars.fit(train_df['text'])\n",
    "#xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "#xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "#\n",
    "## Naive Bayes on Character Level TF IDF Vectors\n",
    "#accuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "#print (\"NB, CharLevel Vectors: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
