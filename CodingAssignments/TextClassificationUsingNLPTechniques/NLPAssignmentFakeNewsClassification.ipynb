{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #The Following function cleans stop words and/or punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"fake_or_real_news_training.csv\")\n",
    "test_df = pd.read_csv(\"fake_or_real_news_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Daniel Greenfield, a Shillman Journalism Fello...\n",
       "1    Google Pinterest Digg Linkedin Reddit Stumbleu...\n",
       "2    U.S. Secretary of State John F. Kerry said Mon...\n",
       "3    — Kaydee King (@KaydeeKing) November 9, 2016 T...\n",
       "4    It's primary day in New York and front-runners...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[0:5]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Daniel, Greenfield, Shillman, Journalism, Fel...\n",
       "1    [Google, Pinterest, Digg, Linkedin, Reddit, St...\n",
       "2    [US, Secretary, State, John, F, Kerry, said, M...\n",
       "3    [—, Kaydee, King, KaydeeKing, November, 9, 201...\n",
       "4    [primary, day, New, York, frontrunners, Hillar...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['text'].head(5).apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_title_text = train_df['title'] + \". \" + train_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_title_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=.33)\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df['text'], train_df['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "#encoder = preprocessing.LabelEncoder()\n",
    "#train_y = encoder.fit_transform(train_y)\n",
    "#valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    #predictions = predict_labels(classifier, feature_vector_valid, is_neural_net)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_to_csv(df_test, df_predictions, filename):\n",
    "    # Output the predictions into a csv file\n",
    "    columns = ['ID','label']\n",
    "    df_submission = pd.DataFrame(columns=columns)\n",
    "    #to_predict_features=pd.read_csv('TestSetValues.csv',parse_dates=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    df_predictions = df_predictions.reset_index(drop=True)\n",
    "    df_submission = df_submission.reset_index(drop=True)\n",
    "    df_submission['ID'] = df_test['ID']\n",
    "    df_submission['label'] = df_predictions[0]\n",
    "    df_submission.to_csv(filename, sep=\",\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_CountVectorizer(to_predict_df):\n",
    "    # create a count vectorizer object \n",
    "    #count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words='english')\n",
    "    count_vect = CountVectorizer(analyzer='text_process')\n",
    "    count_vect.fit(train_df['text'])\n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain_count =  count_vect.transform(train_x)\n",
    "    xvalid_count =  count_vect.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Count Vectors\n",
    "    accuracy = train_model(MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "    print (\"NB, Count Vectors: \", accuracy)\n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_count = count_vect.transform(test_x)\n",
    "    predictions = predict_labels(MultinomialNB(), xtrain_count, train_y, xtest_count)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"CountVectorizer.prediction.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_WordLevelTfidfVectorizer(to_predict_df):\n",
    "    # create a count vectorizer object \n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "    tfidf_vect.fit(train_df['text'])\n",
    "    \n",
    "    xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "    # Naive Bayes on Word Level TF IDF Vectors\n",
    "    accuracy = train_model(MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "    print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf = tfidf_vect.transform(test_x)\n",
    "    predictions = predict_labels(MultinomialNB(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"WordLevelTfidfVectorizer.prediction.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_NGramLevelTfidfVectorizer(to_predict_df):\n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    tfidf_vect_ngram.fit(train_df['text'])\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print (\"NB, N-Gram Vectors: \", accuracy)  \n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "    predictions = predict_labels(MultinomialNB(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"NGramLevelTfidfVectorizer.prediction.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_CharLevelTfidfVectorizer(to_predict_df):\n",
    "    # characters level tf-idf\n",
    "    tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    tfidf_vect_ngram_chars.fit(train_df['text'])\n",
    "    xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "    xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "    # Naive Bayes on Character Level TF IDF Vectors\n",
    "    accuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "    print (\"NB, CharLevel Vectors: \", accuracy)   \n",
    "\n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_vect_ngram_chars = tfidf_vect_ngram_chars.transform(test_x)\n",
    "    predictions = predict_labels(MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_vect_ngram_chars)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"CharLevelTfidfVectorizer.prediction.csv\")\n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text_process is not a valid tokenization scheme/analyzer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-8b5342a82406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_using_CharLevelTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpredict_using_naive_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-8b5342a82406>\u001b[0m in \u001b[0;36mpredict_using_naive_bayes\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_using_naive_bayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_using_CountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_using_WordLevelTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-ca47447918b0>\u001b[0m in \u001b[0;36mpredict_using_CountVectorizer\u001b[0;34m(to_predict_df)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words='english')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text_process'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# transform the training and validation data using count vectorizer object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \"\"\"\n\u001b[0;32m--> 836\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0manalyze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m         \u001b[0mj_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_int_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mbuild_analyzer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n\u001b[0;32m--> 270\u001b[0;31m                              self.analyzer)\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: text_process is not a valid tokenization scheme/analyzer"
     ]
    }
   ],
   "source": [
    "def predict_using_naive_bayes():\n",
    "    predictions = predict_using_CountVectorizer(test_df)\n",
    "    \n",
    "    predictions = predict_using_WordLevelTfidfVectorizer(test_df)\n",
    "    \n",
    "    predictions = predict_using_NGramLevelTfidfVectorizer(test_df)\n",
    "    \n",
    "    predictions = predict_using_CharLevelTfidfVectorizer(test_df)\n",
    "    \n",
    "predict_using_naive_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using Logistic Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_CountVectorizer_logit(to_predict_df):\n",
    "    # create a count vectorizer object \n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words='english')\n",
    "    count_vect.fit(train_df['text'])\n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain_count =  count_vect.transform(train_x)\n",
    "    xvalid_count =  count_vect.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Count Vectors\n",
    "    accuracy = train_model(LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "    print (\"LR, Count Vectors: \", accuracy)\n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_count = count_vect.transform(test_x)\n",
    "    predictions = predict_labels(LogisticRegression(), xtrain_count, train_y, xtest_count)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"CountVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_WordLevelTfidfVectorizer_logit(to_predict_df):\n",
    "    # create a count vectorizer object \n",
    "    \n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "    tfidf_vect.fit(train_df['text'])\n",
    "    \n",
    "    xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "    # Naive Bayes on Word Level TF IDF Vectors\n",
    "    accuracy = train_model(LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "    print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf = tfidf_vect.transform(test_x)\n",
    "    predictions = predict_labels(LogisticRegression(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"WordLevelTfidfVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_NGramLevelTfidfVectorizer_logit(to_predict_df):\n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    tfidf_vect_ngram.fit(train_df['text'])\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print (\"LR, N-Gram Vectors: \", accuracy)  \n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "    predictions = predict_labels(LogisticRegression(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"NGramLevelTfidfVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions\n",
    "\n",
    "def predict_using_CharLevelTfidfVectorizer_logit(to_predict_df):\n",
    "    # characters level tf-idf\n",
    "    tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    tfidf_vect_ngram_chars.fit(train_df['text'])\n",
    "    xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "    xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "    # Naive Bayes on Character Level TF IDF Vectors\n",
    "    accuracy = train_model(LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "    print (\"LR, CharLevel Vectors: \", accuracy)   \n",
    "\n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_vect_ngram_chars = tfidf_vect_ngram_chars.transform(test_x)\n",
    "    predictions = predict_labels(LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_vect_ngram_chars)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"CharLevelTfidfVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_logistic():\n",
    "    predictions = predict_using_CountVectorizer_logit(test_df)\n",
    "    \n",
    "    predictions = predict_using_WordLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "    predictions = predict_using_NGramLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "    predictions = predict_using_CharLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "predict_using_logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_NGramLevelTfidfVectorizer_svm(to_predict_df):\n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    tfidf_vect_ngram.fit(train_df['text'])\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "    \n",
    "    # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print (\"LR, N-Gram Vectors: \", accuracy)  \n",
    "    \n",
    "    test_x = to_predict_df['text']\n",
    "    xtest_tfidf_ngram = tfidf_vect_ngram.transform(test_x)\n",
    "    predictions = predict_labels(svm.SVC(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "    #predictions.shape\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    write_predictions_to_csv(to_predict_df, df_predictions, \"NGramLevelTfidfVectorizer.prediction.logit.csv\")\n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_svm():\n",
    "    predictions = predict_using_NGramLevelTfidfVectorizer_svm(test_df)\n",
    "    \n",
    "    #predictions = predict_using_WordLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "    #predictions = predict_using_NGramLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "    #predictions = predict_using_CharLevelTfidfVectorizer_logit(test_df)\n",
    "    \n",
    "predict_using_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word level tf-idf\n",
    "#tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "#tfidf_vect.fit(train_df['text'])\n",
    "#xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "#xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "#\n",
    "## Naive Bayes on Word Level TF IDF Vectors\n",
    "#accuracy = train_model(MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "#print (\"NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ngram level tf-idf \n",
    "#tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "#tfidf_vect_ngram.fit(train_df['text'])\n",
    "#xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "#xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "#\n",
    "## Naive Bayes on Ngram Level TF IDF Vectors\n",
    "#accuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "#print (\"NB, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## characters level tf-idf\n",
    "#tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "#tfidf_vect_ngram_chars.fit(train_df['text'])\n",
    "#xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "#xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "#\n",
    "## Naive Bayes on Character Level TF IDF Vectors\n",
    "#accuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "#print (\"NB, CharLevel Vectors: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
